<html>
<head>
    <meta charset="utf-8"/>
<meta name="description" content=""/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>房价预测 | Dasein</title>

<link rel="shortcut icon" href="https://stdasein.life/favicon.ico?v=1598361688876">

<link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://stdasein.life/styles/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css">

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dockerfile.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dart.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/moment@2.27.0/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/js/bootstrap.min.js"></script>
<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->



    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css">
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <nav class="navbar navbar-expand-lg">
    <div class="navbar-brand">
        <img class="user-avatar" src="/images/avatar.png" alt="头像">
        <div class="site-name gt-c-content-color-first">
            Dasein
        </div>
    </div>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
                <div class="nav-item">
                    
                        <a href="/" class="menu gt-a-link">
                            首页
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/archives" class="menu gt-a-link">
                            归档
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/tags" class="menu gt-a-link">
                            标签
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/post/about" class="menu gt-a-link">
                            关于
                        </a>
                    
                </div>
            
        </div>
        <div style="text-align: center">
            <form id="gridea-search-form" style="position: relative" data-update="1598361688876" action="/search/index.html">
                <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
                <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
        </div>
    </div>
</nav>

    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    房价预测
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        · 2020-08-18 ·
                    </time>
                    
                        <a href="https://stdasein.life/tag/9S8F6aIMy/" class="post-tags">
                            # 数据分析
                        </a>
                    
                </div>
                <div class="post-content">
                    <h1 id="项目背景">项目背景</h1>
<p>房价预测是kaggle平台上的一个线性回归问题，要求参赛者根据房屋的属性预测房价。</p>
<h1 id="数据概况">数据概况</h1>
<p>数据来源：<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview">kaggle</a></p>
<p>数据内容：数据集含有79个特征（房子的各种属性：面积、有无游泳池、卧室数量等）以及1个标签（房价）<br>
<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">数据详情</a></p>
<h1 id="框架">框架</h1>
<p><img src="https://stdasein.life/post-images/1597881995249.png" alt="" width="400" height="400" loading="lazy"><br>
数据探索：对数据集进行初步观察以获得整体概念。<br>
数据清理：对数据集可能存在的缺失值和异常值进行处理。<br>
特征工程：在理解现有特征的基础上尝试构建新特征，并进行规整、选择。<br>
模型构建：构建预测模型、对模型评分并选择最优的模型进行预测。</p>
<h1 id="数据探索分析">数据探索分析</h1>
<p>本阶段的目的是回答以下问题：<br>
1、特征的数据类型是什么？<br>
2、特征都有什么特点？<br>
3、哪些特征还不能直接使用，需要做进一步的处理？</p>
<p>引进常用的库来满足数据分析和可视化的需求：</p>
<pre><code>#基本环境设置
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas_profiling as pp
#加载训练集和测试集
path1 = 'train.csv'
path2 = 'test.csv'
train_data = pd.read_csv(path1)
test_data = pd.read_csv(path2)
train_data.head()
test_data.head()
</code></pre>
<p>为了方便操作，分离特征与标签并将训练集与测试集合并：</p>
<pre><code>#分离特征和标签
X_train = train_data.drop(['SalePrice'],axis=1)
y_train = train_data['SalePrice']
#合并
X_data = X_train.append(test_data)
#id列对预测没有意义，可舍去
X_data.drop(['Id'],axis=1,inplace=True)
</code></pre>
<p>使用pandas_profiling进行数据探索，对于数据集的每一列，pandas_profiling会提供以下统计信息：<br>
1、概要：数据类型，唯一值，缺失值，内存大小<br>
2、分位数统计：最小值、最大值、中位数、Q1、Q3、最大值，值域，四分位<br>
3、描述性统计：均值、众数、标准差、绝对中位差、变异系数、峰值、偏度系数<br>
4、最频繁出现的值，直方图/柱状图<br>
5、相关性分析可视化：突出强相关的变量，Spearman, Pearson矩阵相关性色阶图</p>
<pre><code>report = pp.ProfileReport(X_data)
report
</code></pre>
<p><strong>Overview</strong><br>
Overview部分提供了数据集的整体信息：缺失值数量、重复行数和数据的类型。<br>
<img src="https://stdasein.life/post-images/1597715512282.png" alt="" loading="lazy"><br>
可见，数据集没有重复的行或列，但有13,965个缺失值需要处理，且数据主要为文本型（46）和数值型（33），这在后续分析时要我们进一步区分。</p>
<p><strong>Variables</strong><br>
variables部分会显示各特征的详细情况，例如、缺失值数量、唯一值数量、均值、标准差等。<br>
<img src="https://stdasein.life/post-images/1597715778189.png" alt="" loading="lazy"><br>
以LotArea特征为例，可知其数据类型为数值型，没有缺失值，但标准差相较于其他特征要大得多，意味着可能存在异常值。</p>
<p><strong>Correlations</strong><br>
Correlations衡量各变量之间的相关性，为特征工程提供支持。<br>
<img src="https://stdasein.life/post-images/1597716013886.png" alt="" loading="lazy"></p>
<h1 id="数据清理">数据清理</h1>
<p>错误的数据会使模型的预测发生偏差，所以在开始预测之前要先进行数据清理，处理数据中的噪音。<br>
<img src="https://stdasein.life/post-images/1597882228594.png" alt="" width="400" height="400" loading="lazy"><br>
数据清理可分为三个步骤：处理缺失值、处理异常值和处理重复值。我们在数据探索部分已经知道数据集不存在重复，所以这里只需关注前两种情况。</p>
<h2 id="缺失值处理">缺失值处理</h2>
<p>缺失值处理主要有三种方法：<br>
1、dropna：将含有缺失值的整列/行舍弃；<br>
2、fillna：用该列/行的均值、中位数或众数等填充缺失值；<br>
3、模型预测：可用knn等模型预测缺失值；<br>
模型预测方法相对来说会有更高的可靠性，但由于本数据集的数据缺失大部分都有单一且明确的原因，所以用fillna的方法会更加方便灵活。</p>
<pre><code>#查看缺失值占比
def CountNull(data):
    null_count = data.isnull().sum().sort_values(ascending = False)
    ratio = null_count/len(data)
    nulldata = pd.concat([null_count,ratio],axis = 1, keys=['count','ratio'])
    return nulldata[ratio&gt;0]

CountNull(X_data)
</code></pre>
<p><img src="https://stdasein.life/post-images/1597883278080.png" alt="" width="200" loading="lazy"><br>
<img src="https://stdasein.life/post-images/1597883294548.png" alt="" width="200" loading="lazy"></p>
<p>根据官方提供的data_description文本，有些特征出现缺失值是因为房子没有该特征（例如有的房子没有游泳池），对于这一类可以选择填入None，如果是数值类型的就填入0；除此之外，对于缺失值比较少的特征（缺失值仅有1或2个），可以直接填充众数。</p>
<pre><code>#选择缺失值数量为1或2的特征
feature_fill_mode = CountNull(X_data)[(CountNull(X_data)['count'] == 1) | (CountNull(X_data)['count'] == 2)].index
#因为同一个社区的LotFrontage（街道到房屋的距离）应该比较接近，所以填充同社区的中位数
feature_fill_neig = ['LotFrontage']
#选择要填入none的特征
feature_fill_none = ['MasVnrType','BsmtFinType1','BsmtFinType2','BsmtQual','BsmtExposure','BsmtCond',
                    'GarageType','GarageFinish','GarageQual','GarageCond','FireplaceQu','Fence','Alley',
                    'MiscFeature','PoolQC','MSZoning']
#选择要填入0的特征
feature_fill_zero = ['MasVnrArea','GarageYrBlt']
</code></pre>
<p>对特征的缺失值进行填充：</p>
<pre><code>for col in feature_fill_mode:
X_data[col].fillna(X_data[col].mode()[0], inplace=True)

for col in feature_fill_none:
    X_data[col].fillna('None',inplace=True)

for col in feature_fill_zero:
    X_data[col].fillna(0,inplace=True)
    
X_data['LotFrontage'] = X_data.groupby(&quot;Neighborhood&quot;)['LotFrontage'].transform(lambda x: x.fillna(x.median()))    
CountNull(X_data)
</code></pre>
<p>所有缺失值均已填充完毕。</p>
<h2 id="异常值处理">异常值处理</h2>
<p>（1）观察法<br>
所谓观察法，就是通过描述性统计分析和可视化来寻找可能存在的异常值。</p>
<pre><code>#筛选数值型特征
num_features = X_data.select_dtypes('number')
num_names = num_features.columns
des = X_data[num_names].describe()
#按特征的标准差排序
des.sort_values(by = 'std',ascending = False, axis = 1)
</code></pre>
<p><img src="https://stdasein.life/post-images/1597627987467.png" alt="" loading="lazy"><br>
可见，LotArea的标准差要远大于其他特征，且极差也非常大，应进一步研究是否存在异常值。</p>
<pre><code>ax = sns.scatterplot(x=&quot;LotArea&quot;, y=&quot;SalePrice&quot;, data=train_data)
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://stdasein.life/post-images/1597628238676.png" alt="" width="300" loading="lazy"></figure>
<pre><code>#用中位数替换异常值
X_data.loc[X_data['LotArea']&gt;150000,['LotArea']]=9453
</code></pre>
<p>（2）使用IsolationForest剔除异常值<br>
IsolationForest是适用于连续数据的无监督异常检测方法，该算法会将数据只占很少量、数据特征值和正常数据差别很大的值视为异常值。</p>
<pre><code>#因为只需剔除训练集的异常值，所以暂时将训练、测试集分离
X_train = X_data.iloc[:1460,:]
X_test = X_data.iloc[1460:,:]
X_train.shape
(1460, 79)

#建立算法模型：
from sklearn.ensemble import IsolationForest
iso = IsolationForest()
#剔除异常值：算法将-1表示为异常值，所以mask表示不含有异常值的行
yhat = iso.fit_predict(X_train[num_names])
mask = yhat != -1
X_train, y_train = X_train.values[mask, :], y_train.values[mask]
X_train = pd.DataFrame(X_train,columns = X_data.columns)
X_train.shape
(1380, 79)
</code></pre>
<p>算法剔除了80行包含有异常值的数据。</p>
<pre><code>#将训练集和测试集合并
X_data_removed = pd.concat([X_train,X_test],axis = 0)
X_data_removed[num_names] = X_data_removed[num_names].astype(int)
X_data_removed.reset_index(drop=True, inplace=True)
</code></pre>
<h1 id="特征工程">特征工程</h1>
<p>特征工程是机器学习的核心环节，好的特征工程可以有效提高预测的精度，其主要步骤如下图所示：<br>
<img src="https://stdasein.life/post-images/1597820518048.png" alt="" width="400" loading="lazy"></p>
<h2 id="特征类型">特征类型</h2>
<p>因为不同的数据类型有不同的处理方式，所以第一步是识别各特征的数据类型：<br>
1、数值型：连续、离散<br>
2、类别型</p>
<pre><code>#筛选全部数值型特征
numeric_feature = X_data_removed.select_dtypes('number')
numeric_names = numeric_feature.columns
numeric_name
</code></pre>
<p><img src="https://stdasein.life/post-images/1597822957068.png" alt="" width="400" loading="lazy"><br>
根据特征的说明文档，MSSubClass（房屋类型）、OverallQual（房屋质量评分）、OverallCond（房屋环境评分）应划分为类别型特征：</p>
<pre><code>X_data_removed[['MSSubClass','OverallQual','OverallCond']] = X_data_removed[['MSSubClass', 'OverallQual','OverallCond']].astype(str)
</code></pre>
<h2 id="特征构建">特征构建</h2>
<p>我们可以利用原有的特征构建新特征：</p>
<pre><code>#构建房屋总面积
X_data_removed['TotalHouseArea'] = X_data_removed['TotalBsmtSF'] + X_data_removed['1stFlrSF'] + X_data_removed['2ndFlrSF']
#构建房屋翻修年限
X_data_removed['YearsSinceRemodel'] = X_data_removed['YrSold'] - X_data_removed['YearRemodAdd']
#构建房屋建造年限
X_data_removed['YrBlt'] = X_data_removed['YrSold'] - X_data_removed['YearBuilt'] 
#构建总浴室数量
X_data_removed['Total_Bathrooms'] = (X_data_removed['FullBath'] + (0.5 * X_data_removed['HalfBath']) +
                            X_data_removed['BsmtFullBath'] + (0.5 * X_data_removed['BsmtHalfBath']))
#构建门廊总面积
X_data_removed['Total_porch_sf'] = (X_data_removed['OpenPorchSF'] + X_data_removed['3SsnPorch'] +
                            X_data_removed['EnclosedPorch'] + X_data_removed['ScreenPorch'] +
                            X_data_removed['WoodDeckSF'])
</code></pre>
<h2 id="特征规整">特征规整</h2>
<p>1、标准化<br>
标准化使不同维度上的特征在数值上更具可比性，能够提升模型的精度。<br>
这里只对数值型数据进行标准化处理：</p>
<pre><code>#筛选数值型数据
num = X_data_removed.select_dtypes('number')
num_names = num.columns
#引入标准化工具并进行标准化
from sklearn.preprocessing import MinMaxScaler
#创建对象，限定范围（1，2）
scaler = MinMaxScaler(feature_range=(1, 2))
X_data_removed[num_names] = scaler.fit_transform(X_data_removed[num_names])
X_data_removed[numeric_names].head()
</code></pre>
<p><img src="https://stdasein.life/post-images/1597991752718.png" alt="" width="500" loading="lazy"><br>
标准化后，数值的大小被控制在1到2之间。</p>
<p>2、非正态分布转换<br>
正态分布的数据能更好地提高模型的准确度，因此我们要对非正态分布的数据进行转换。</p>
<pre><code>from scipy.stats import norm, skew    
from scipy import stats,special

#查看特征值的偏度分布
def sKewNess(data):
    skewed = data.apply(lambda x:skew(x)).sort_values(ascending=False)
    skewness = pd.DataFrame({'skew':skewed})
    return skewness[skewness['skew'].abs()&gt;0.75]

sKewNess(X_data_removed[num_names])
</code></pre>
<p><img src="https://stdasein.life/post-images/1597649891150.png" alt="" width="300" loading="lazy"><br>
可见，很多特征都不是正态分布的，我们可以尝试用box-cox对特征进行转换：</p>
<pre><code>#引用sklearn的PowerTransformer工具
from sklearn.preprocessing import PowerTransformer
#box-cox方法只对正数生效，因而要配合上面的MinMaxScaler使用
power = PowerTransformer(method='box-cox')
X_data_removed[numeric_names] = power.fit_transform(X_data_removed[numeric_names])
    
sKewNess((X_data_removed[num_names]))
</code></pre>
<p><img src="https://stdasein.life/post-images/1597842618747.png" alt="" width="300" loading="lazy"><br>
得到了一定的改善，但部分数据因为自身的种种局限性仍保持较大的偏度。</p>
<pre><code>#查看标签偏度
sns.distplot(y_train)
</code></pre>
<p><img src="https://stdasein.life/post-images/1597651434520.png" alt="" width="400" loading="lazy"><br>
显然，房价不是正态分布。</p>
<pre><code>#对房价进行正态分布转换
y_train = np.log1p(y_train)
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://stdasein.life/post-images/1597651550033.png" alt="" width="400" loading="lazy"></figure>
<p>3、编码与哑变量<br>
许多算法模型只支持数值型的数据，因此我们需要把类别型数据转换成数值形式。<br>
<img src="https://stdasein.life/post-images/1597992239924.png" alt="" width="400" loading="lazy"><br>
类别型数据又可以细分为两种类型：无序型（例如性别：男、女两个变量是独立的，没有顺序关系）和有序型（例如质量评分：1分和2分有顺序关系）</p>
<pre><code>#筛选类别型数据
X_data_removed[['OverallQual','OverallCond']] = X_data_removed[['OverallQual','OverallCond']].astype(int)
cat_feature = X_data_removed.select_dtypes('object')
cat_names = cat_feature.columns
cat_names
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://stdasein.life/post-images/1597910706940.png" alt="" width="500" loading="lazy"></figure>
<pre><code>#区分有序、无序数据
ordinal_feature= ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC',
                'FireplaceQu','GarageQual','GarageCond','PoolQC']
nominal_feature = [i for i in cat_names if i not in ordinal_feature]
</code></pre>
<p>诸如ExterQual（对外部环境的评分：poor、good、excellent）等特征都可归为有序特征，剩下的即为无序特征。</p>
<pre><code>#排序
rank_dict = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}
for i in ordinal_feature:
    X_data_removed[i] = X_data_removed[i].map(rank_dict)
X_data_removed[ordinal_feature].head()
</code></pre>
<figure data-type="image" tabindex="4"><img src="https://stdasein.life/post-images/1597911169498.png" alt="" width="500" loading="lazy"></figure>
<pre><code>#用get_dummies函数将剩下的非数值型特征转化为哑变量
X_data_removed = pd.get_dummies(X_data_removed)
X_data_removed.head()
</code></pre>
<p><img src="https://stdasein.life/post-images/1597911297096.png" alt="" width="500" loading="lazy"><br>
上图只截取了SaleCondition生成的部分哑变量样本。</p>
<pre><code>X_data_removed.shape
(2832, 283)
</code></pre>
<p>可见，使用get_dummies生成哑变量后，维度由79增长到了283，维度越高模型越容易过拟合。</p>
<h2 id="特征选择">特征选择</h2>
<p>特征数量过多可能导致模型过拟合，因此我们可以筛选出最重要的特征以降低特征维度，从而提升模型精度。<br>
<img src="https://stdasein.life/post-images/1597992410626.png" alt="" width="400" loading="lazy"><br>
特征选择的方法有：相关系数法、互信息法、PCA主成分分析和递归特征消除法等，这里我们仅选择前两种进行分析。</p>
<p>定义评分函数<br>
定义评分函数方便我们比较不同选择方法的效果：</p>
<pre><code>#环境设置
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from numpy import mean

#分离训练集、测试集
X_train = X_data_removed.iloc[:len(y_train),:]
X_test = X_data_removed.iloc[len(y_train):,:]

#定义评分函数
model = RandomForestRegressor(random_state = 42)
def evaluate(X_train,y_train):
    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)
    scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', 
                        cv=cv, n_jobs=-1, error_score='raise')
    return mean(scores)

#进行特征选择前的基本模型评分
evaluate(X_train, y_train)
-0.017256
</code></pre>
<p>定义特征选择函数<br>
相关系数法和互信息法都可以通过SelectKBest实现，因此我们可以定义函数方便转换：</p>
<pre><code>from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
from sklearn.feature_selection import mutual_info_regression

def select_features(X_train, y_train, X_test,func,k):
    # k:283个特征中选取k个,score_func:调用的方法
    fs = SelectKBest(score_func=func, k=k)
    fs.fit(X_train, y_train)
    # 转换训练集
    X_train_fs = fs.transform(X_train)
    # 转换测试集
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
</code></pre>
<p>（1）相关系数法</p>
<pre><code>#填入f_regression来调用相关系数法
X_train_fsc, X_test_fsc, fsc = select_features(X_train, y_train, X_test,f_regression,280)
evaluate(X_train_fsc, y_train)
-0.017265
</code></pre>
<p>相较于选择前的 -0.017256，评分略有下降。</p>
<p>（2）互信息法</p>
<pre><code>X_train_fsm, X_test_fsm, fsm = select_features(X_train, y_train, X_test,mutual_info_regression,280)
evaluate(X_train_fsm, y_train)
-0.017217
</code></pre>
<p>评分上升了，我们可以用GridSearchCV继续寻找最优的特征数量：</p>
<pre><code>from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)
model = RandomForestRegressor()
fs = SelectKBest(score_func=mutual_info_regression)
pipeline = Pipeline(steps=[('sel',fs), ('rf', model)])
grid = dict()
#sel表示SelectKBest，__k表示SelectKBest的参数k；特征选取范围（253-283）
grid['sel__k'] = [i for i in range(X_train.shape[1]-30, X_train.shape[1]+1)]
# 定义GridSearchCV
search = GridSearchCV(pipeline, grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv)
# 运行GridSearchCV
results = search.fit(X_train, y_train)
print('Best MSE: %.3f' % results.best_score_)
print('Best Config: %s' % results.best_params_)
Best MSE: -0.017
Best Config: {'sel__k': 282}
</code></pre>
<p>可见，选取282个特征时评分最高，但维度仍然较高，接下来尝试一下PCA法。</p>
<p>（3）PCA（主成分分析）</p>
<pre><code>from sklearn.decomposition import PCA
#指定降维后的主成分方差和比例为99%
pca_model = PCA(n_components = 0.99)
X_train_de = pca_model.fit_transform(X_train)
X_test_de = pca_model.transform(X_test)
X_train_de.shape
(1383, 124)

evaluate(X_train_de,y_train)
-0.017633
</code></pre>
<p>PCA法保留了124个特征，但评分并不好。</p>
<p>最终，我们选择评分较好的互信息法的282个特征用于预测：</p>
<pre><code>X_train_mb,X_test_mb,fs_mb =select_features(X_train, y_train, X_test,mutual_info_regression,282)
</code></pre>
<h1 id="模型构建">模型构建</h1>
<pre><code>from numpy import mean, std
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold
from sklearn.model_selection import RepeatedKFold
from sklearn.ensemble import StackingRegressor
</code></pre>
<p>使用堆叠法：<br>
堆叠法是一种整合多个模型为一个集成模型从而提高预测精度的方法。集成模型一般分两层，第一层会选取几个强力模型作为基本模型，如随机森林、XGboost等，第二层则会选择比较简单的模型，如linearRegressor,详情见：<br>
<a href="https://blog.csdn.net/wstcjf/article/details/77989963">详解stacking过程</a></p>
<pre><code>def get_stacking():
    # 构建基本模型
    level0 = list()
    level0.append(('lgbm', LGBMRegressor()))
    level0.append(('Forest', RandomForestRegressor()))
    level0.append(('svm', SVR()))
    level0.append(('GDB', GradientBoostingRegressor()))
    level0.append(('XGB', XGBRegressor(objective ='reg:squarederror')))
    # 构建次级模型
    level1 = KernelRidge()
    # define the stacking ensemble
    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=10)
    return model
</code></pre>
<p>引进其它模型：</p>
<pre><code>def get_models():
    models = dict()
    models['knn'] = KNeighborsRegressor()
    models['cart'] = DecisionTreeRegressor()
    models['svm'] = SVR()
    models['k'] = KernelRidge()
    models['Forest'] = RandomForestRegressor()
    models['GDB'] = GradientBoostingRegressor()
    models['Extra'] = ExtraTreesRegressor()
    models['XGB'] = XGBRegressor(objective ='reg:squarederror')
    models['LGBM'] = LGBMRegressor()
    models['stacking'] = get_stacking()
    return models
</code></pre>
<p>定义评分函数：</p>
<pre><code>def evaluate_model(model):
    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
    scores = cross_val_score(model, X_train_mb, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')
    return scores
</code></pre>
<p>模型比较：</p>
<pre><code>models = get_models()
#遍历，评估各模型性能
results, names = list(), list()
for name, model in models.items():
    scores = evaluate_model(model)
    results.append(scores)
    names.append(name)
    print('&gt;%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
</code></pre>
<blockquote>
<p>knn -0.024 (0.005)<br>
cart -0.037 (0.007)<br>
svm -0.013 (0.003)<br>
k -0.017 (0.006)<br>
Forest -0.017 (0.004)<br>
GDB -0.014 (0.004)<br>
Extra -0.015 (0.004)<br>
XGB -0.014 (0.003)<br>
LGBM -0.015 (0.004)<br>
stacking -0.012 (0.003)</p>
</blockquote>
<p>集成模型的表现最优异，我们将用它作为预测模型：</p>
<pre><code>model = get_stacking()
model.fit(X_train_mb,y_train)
result = model.predict(X_test_mb)
#因为之前将房价进行了正态分布转换，现在要进行还原
result = np.expm1(result)

#导出结果
r=pd.DataFrame()
r['Id']=test_data['Id']
r['SalePrice']=result
#将预测结果导出为csv文件
r.to_csv(&quot;C:\\Users\\Steven\\Desktop\\Kaggle\\housing2.csv&quot;, index = False)</code></pre>

                </div>
            </article>
        </div>

        
            <div class="next-post">
                <div class="next gt-c-content-color-first">下一篇</div>
                <a href="https://stdasein.life/post/pa-chong-ji-ben-liu-cheng-yi-dou-ban-dian-ying-top250-wei-li/" class="post-title gt-a-link">
                    爬虫基本流程——爬取豆瓣电影top250
                </a>
            </div>
        

        

        

        
            <script src='https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js'></script>

<style>
	div#vcomments{
		width:60%;
		max-width: 1000px;
		padding: 2.5%
	}
</style>


	<div id="vcomments"></div>

<script>
	new Valine({
		el: '#vcomments',
		appId: 'suhtzqRXVOn82V3a1GwIBG57-gzGzoHsz',
		appKey: '1WhE54RtvjL0wzsFGqJ101o6',
		avatar: 'monsterid',
		pageSize: 5,
		recordIp: false,
		placeholder: 'Just Go Go',
		visitor: false,
	});
</script>

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">Freiheit als Autonomie</div>
    <div class="social-container">
        
            
        
            
        
            
        
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
    <div>
        Theme by <a href="https://imhanjie.com/" target="_blank">imhanjie</a>, Powered by <a
                href="https://github.com/getgridea/gridea" target="_blank">Gridea | <a href="https://stdasein.life/atom.xml" target="_blank">RSS</a></a>
    </div>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
</div>
</body>
</html>
