{"posts":[{"title":"电商指标分析","content":"目录： 一、数据概况 二、提出问题 三、分析思路 四、数据清理 五、数据分析 六、增长建议 一、数据概况 数据来源： 阿里天池 数据内容： 数据集提供了“双十一”期间部分卖家及其相应新买家的数据，包括这些新买家在前半年的购物行为日志，以及基本用户信息，所以数据集提供的用户均在”双十一“期间有购买行为。 二、提出问题 用户方面： 网站流量状况如何，在获客方面是否存在问题？ 新用户留存状况，用户忠诚度如何？ 获取营收状况？放弃购买的用户在哪个环节流失？ 怎样通过用户分层进行精细化运营？ 商品方面： 哪些商品销量更高？ 商品之间是否存在购买的关联性？ 卖家方面： 成功的卖家有什么特点？是否有值得借鉴之处？ 三、分析思路 使用人货场的分析框架并结合AARRR指标体系。 采用多维度拆解分析方法对问题进行拆解，用假设检验分析法、对比分析法和kmeans模型具体分析用户使用流程及具体业务指标中的问题。 四、数据清理 数据清理的步骤： 1.数据加载、合并 使用pandas载入数据集： import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns path_user_info = 'user_info_format1.csv' path_user_log = 'user_log_format1.csv' user_info = pd.read_csv(path_user_info) user_log = pd.read_csv(path_user_log) 1.1 查看数据 分别查看user_info和user_log的内容： （1）user_info #查看user_info前五条数据 user_info.head() user_info包含了用户的基本信息： user_id； age_range: 1 for &lt;18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for &gt;= 50; 0 and NULL for unknown； gender: 0 for female, 1 for male, 2 and NULL for unknown; （2）user_log #查看user_log前五条内容 user_log.head() user_log是用户的行为记录： item_id：商品的编号 cat_id：商品的种类 seller_id：商家的id brand_id：商品的品牌 time_tamp：行为发生的时间（format:mmdd) action_type：0表示点击，1表示加入购物车，2表示购买，3表示收藏 1.2 合并数据 为了方便分析，将user_info和user_log进行合并： user_full = pd.merge(user_log,user_info) user_full.head() #查看数据量 user_full.shape (54925330, 9) 2.缺失值处理 2.1 查看缺失值分布 #计算缺失值的数量 count_null = user_full.isnull().sum().sort_values(ascending=False) #缺失值占比 ratio = count_null/len(user_full) count_ratio = pd.concat([count_null,ratio],axis=1,keys = ['count','ratio']) count_ratio[count_ratio.ratio&gt;0] 可见，缺失值的占比较小，且只分布在性别、年龄和品牌三列。 2.2 填充缺失值 因为数据集里性别未知表示为NaN或2，所以我们将缺失值统一填充为2；同理，将年龄缺失值填充为0。 #填充年龄、性别的缺失值 user_full['age_range'].fillna(0,inplace=True) user_full['gender'].fillna(2,inplace=True) #对品牌的缺失值填充‘Missing' user_full['brand_id'].fillna('Missing',inplace=True) user_full.isnull().sum() user_id 0 item_id 0 cat_id 0 seller_id 0 brand_id 0 time_stamp 0 action_type 0 age_range 0 ender 0 dtype: int64 3.重复值处理 #选择重复的行 count_dup = user_full[user_full.duplicated()] count_dup.count() user_id 2498641 item_id 2498641 cat_id 2498641 seller_id 2498641 brand_id 2498641 time_stamp 2498641 action_type 2498641 age_range 2498641 gender 2498641 dtype: int64 查询发现，数据集有2,498,641行重复数据，这个数字非常大，不能简单的认为是数据的冗余，可能确实是用户重复操作所导致的，对此我们决定予以保留。 4.时间格式转换 将原本的时间戳转换成更直观的形式。 拆分出月和日，方便后续的分析。 #引入datetime工具库转换时间格式 import datetime as dtimport datetime user_full['time_stamp'] = pd.to_datetime(user_full['time_stamp'],format='%Y%m%d') #拆分月、日 user_full['month'] = user_full['time_stamp'].dt.month.astype(str) user_full['day'] = user_full['time_stamp'].dt.day.astype(str) user_full['year'] = '2015' user_full['date'] = user_full['year'] + user_full['month'] + user_full['day'] user_full['date'] = pd.to_datetime(user_full['date'],format='%Y%m%d') user_full.drop(['time_stamp'],axis=1,inplace=True) user_full.head() 五、数据分析 1.人 这一部分关注点是用户的行为习惯，利用AARRR的部分因素、转化率概念和kmeans模型对用户行为进行分析。 Acquisition：客户获取 指标选取 pv：即Page View，网站浏览量，指页面浏览的次数，用以衡量网站用户访问的网页数量。用户每次打开一个页面便记录1次PV，多次打开同一页面则浏览量累计 uv：即Unique Visitor，独立访客数，指一天内访问某站点的人数，以cookie为依据。1天内同一访客的多次访问只记录为一个访客。 指标分析 按月分组： group_m = user_full.groupby('month') group_m.get_group(5).head() 月流量对比分析： #创建函数计算pv,uv和pv/uv def volumeFlow(x): month = group_m.get_group(x) uv = month['user_id'].nunique() pv = month[month['action_type']== 0]['action_type'].count() return pv,uv,pv/uv keys = ['May','Jun','Jul','Aug','Sep','Oct','Nov'] values = [] #遍历5—11月并将数据放入列表 for i in range(5,12): a = volumeFlow(i) values.append(a) #转化为DataFrame dictionary = dict(zip(keys, values)) df = pd.DataFrame(dictionary,index=['pv','uv','pv/uv']).astype(int) df.columns.name = 'Month' df.index.name = 'indicator' df 可视化： #可视化 pv = df.iloc[0,:].values uv = df.iloc[1,:].values pv_uv = df.iloc[2,:].values x = ['May','Jun','Jul','Aug','Sep','Oct','Nov'] fig,(ax1,ax2,ax3) = plt.subplots(3,1) ax1.plot(x,pv,'o-') ax1.set_ylabel('pv') ax2.plot(x,uv,'.-') ax2.set_ylabel('uv') ax3.plot(x,pv_uv) ax3.set_ylabel('pv/uv') plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35) 分析总结 整体上看，五月到十一月的浏览量呈现出增长的趋势，增长可归因于两方面： 独立访客(uv)的增加： 原因假设：1）新客增加；2）旧客唤醒； 验证思路：对用户按首次使用日期进行群组分析，区分新旧用户，计算新客的增长情况；按用户的来源渠道进行分组，筛选有效的获新/唤醒渠道。（新旧用户的分析将会在留存率环节展开所以在此不过多表述，而来源渠道分析因为资料不足无法进行，在此只提供思路。） 人均浏览量的增加： 原因假设：网页内容优化、推荐算法提升和优惠活动等 十一月份出现流量峰值： 原因假设：双十一促销的影响 假设检验：分析对比双十一前后流量的变化，一般而言，促销过后流量会有一定程度的回落。 收集证据： #抓取11月数据 group_nov = group_m.get_group(11) group_nov = pd.DataFrame(group_nov) #查看11月天数分布 group_nov.day.value_counts() 11 1921587 10 531651 9 213696 8 173739 7 156503 6 146463 5 134096 4 130784 3 108929 1 101456 2 99219 12 11 Name: day, dtype: int64 11月12日的数据量太小，我们将其作为异常值剔除。 group_d = group_nov.groupby('day') #定义函数计算每天的流量 def getDailyVolumeFlow(x): day = group_d.get_group(x) uv = day['user_id'].nunique() pv = day[day['action_type']== 0]['action_type'].count() return pv,uv,pv/uv keys_d = list(range(1,12)) values_d = [] #将每天的流量放进列表中 for i in range(1,12): #不包含第十二天的异常值 a = getDailyVolumeFlow(i) values_d.append(a) #转换为便于观察的DataFrame形式 dictionary_d = dict(zip(keys_d, values_d)) df_day = pd.DataFrame(dictionary_d,index=['pv','uv','pv/uv']).astype(int) df_day 结论： 从统计数字可以看出，越接近双十一，网站的流量越大，而双十一当天的流量甚至比以往一个月的流量都还要大。可见，打造消费文化对消费的强大促进作用。 继续提问：双十一过后的流量还能保持在这个水平吗？会不会透支了用户未来的消费潜力？ 验证思路：可通过零售中促销爆发度和促销衰减度的指标进行分析对比，如果衰减度大于爆发度则有销售透支的现象发生，如果衰减度大于两倍的爆发度，那这个促销活动就是彻底失败了。 受限于手头上的数据，我们无法对双十一之后的流量进行分析。 Retention：客户留存 指标选取 留存率：用户在某段时间内开始使用应用，经过一段时间后，仍然继续使用该应用的用户，被认作是留存用户。这部分用户占当时新增用户的比例即是留存率，会按照每隔1单位时间（例日、周、月）来进行统计。 复购率：复购率就是重复购买的用户占所有存在购买行为用户的比率。 回购率：回购率就是在上个时间窗口有购买行为的同时在下个时间窗口仍然有购买行为的用户数量占上个窗口有购买行为用户的比例。 指标分析 考虑到电商的特殊性，我们以月为单位进行分析。 （1）留存率 留存率的分析思路：首先计算每个月的新增用户数量，再分析后续留存； 留存率=新增用户中登录用户数/新增用户数*100%； 新增用户数：在某个时间段新登录应用的用户数； 登录用户数：登录应用后至当前时间，至少登录过一次的用户数； 以六月份为例，演示留存率计算过程。 第一步：筛选出六月前就已经存在的用户，因为数据集的局限性，我们只能将五月份的数据视为全部的历史数据，所以五月份的用户就是计算新增用户的基准。 #计算基准用户数 may = user_full.loc[user_full['month']==5,:] print('五月份用户数量：%d'%may['user_id'].nunique()) 五月份用户数量：40235 五月份的用户数量为40,235，而在六月份用户中在五月份没出现过的部分，就是新增用户。要注意的是，因为五月份没有前置数据，所以只能将五月份的所有用户都视为新增用户。 有了基准，我们就可以计算出六月的新增用户数了。首先用loc截取六月份的全部数据，然后通过isin筛选新增用户。 #计算六月新增用户数 june = user_full.loc[user_full['month']==6,:] jun_new = june.loc[june['user_id'].isin(may['user_id'])==False,:] print('六月份新增用户数量：%d'%jun_new['user_id'].nunique()) 六月份新增用户数量：17402 六月份新增用户数量为17,402，接下来要计算的留存数量本质上就是在这个数字的基础上查看用户的留存情况，留存数量少于17,402，即出现了用户流失。 为了计算留存数量，我们仍然会使用isin进行筛选，不过方向相反。 retention = [] m_list = [7,8,9,10,11] #遍历计算六月新用户的后续留存状况 for i in m_list: m_next = user_full.loc[user_full['month']==i,:] m_re = m_next.loc[m_next['user_id'].isin(jun_new['user_id'])==True,:] retention.append('%d月份留存: %d'%(i,m_re['user_id'].nunique())) #插入六月份新增用户数 retention.insert(0,'6月份新增用户：%d'%jun_new['user_id'].nunique()) retention ['6月份新增用户：17402', '7月份留存: 11418', '8月份留存: 10977', '9月份留存: 12199', '10月份留存: 13575', '11月份留存: 17402'] 结果显示，六月份新增用户为17,402，八月留存10,977，流失率达到最大，往后几个月逐渐回升。 现在，我们会通过遍历来计算所有月份的留存状况。外层的遍历计算每个月的新增用户数，内层的遍历计算留存数。 month = list(range(5,12)) df_re = pd.DataFrame() #计算每月新增用户 for i in range(len(month)-1): #每次形成一列，放置新增用户数及以后的留存数 count = [0]*len(month) target_month = user_full.loc[user_full['month']==month[i],:] #计算五月份新用户数量，因为五月份无前置月份，所以跳过筛选 if i == 0: new_users = target_month new_users_num = new_users['user_id'].nunique() else: history_month = user_full.loc[user_full['month'].isin(month[:i]),:] new_users = target_month.loc[target_month['user_id'].isin (history_month['user_id'])==False,:] new_users_num = new_users['user_id'].nunique() #储存新用户数量，放在每行的第一个位置 count[0] = new_users_num #同时遍历，j遍历月份，c遍历列表位置 for j,c in zip(range(i + 1,len(month)),range(1,len(month))): next_month = user_full.loc[user_full['month']==month[j],:] next_users = next_month.groupby('user_id')['action_type'].sum().reset_index() #计算留存数 re_next_users = next_users.loc[next_users['user_id'].isin (new_users['user_id'])==True,:] re_next_users_num = re_next_users['user_id'].nunique() #储存留存数 count[c] = re_next_users_num #合并 result = pd.DataFrame({month[i]:count}).T df_re = pd.concat([df_re,result]) df_re.columns = ['新增用户','+1月','+2月','+3月','+4月','+5月','+6月'] df_re.index.name ='月份' df_re 注意：所有月份的留存率在11月都达到100%，这是因为数据集提供的用户均在”双十一“期间有购买行为。 #单独计算11月新增用户数 month_2 = list(range(5,12)) target_month = user_full.loc[user_full['month']==month_2[6],:] history_m_to_d = user_full.loc[user_full['month'].isin(month_2[:6]),:] new_users = target_month.loc[target_month['user_id'].isin (history_m_to_d['user_id'])==False,:] new_users_num_nov = new_users['user_id'].nunique() print('十一月份新增用户数量：%d'%new_users_num_nov) 十一月份新增用户数量：1582 6至11月份，新增用户呈现逐月递减的趋势。 我们把留存数量转化为比率并进行可视化，方便对比分析。 re_table = df_re.divide(df_re['新增用户'],axis=0) re_ratio = re_table.drop(['新增用户'],axis=1) re_ratio.index.name = 'month' re_ratio.columns = ['+1m','+2m','+3m','+4m','+5m','+6m'] #生成热力图 sns.heatmap(re_ratio,annot=True, fmt=&quot;.0%&quot;) 分析小结： 新增用户数量逐月递减： 原因假设：单一获客渠道饱和； 验证思路：根据获客渠道，年龄等不同属性对用户进行群组分析，分析用户成分，如果成分单一，就可能意味着饱和，应尝试开拓新的渠道。 7月的留存率相较于其他月份偏低，仅有59%： 原因假设：网页、app改版，影响用户体验。 验证思路：对流失用户进行问卷调查、AB测试 留存率基本保持在60%以上，高于电商行业标准的5—8%； （2）复购率 复购率分析思路：筛选出具有购买行为的用户，然后通过透视图统计各用户购买次数。 #筛选所有购买行为 user_buy = user_full[user_full['action_type']==2] user_buy.shape (608290, 12) 5至11月间共发生了608,290次购买行为。 用透视表按月份统计每个用户的购买次数: #用透视表按月份统计每个有购买行为的用户的购买次数 pivot = user_buy.pivot_table(index = 'user_id', columns = 'month',values ='action_type', aggfunc='count') pivot.head() 如上图所示，在当月没有购买行为的用户填入NaN。 #为了方便求出复购率，对透视表进行转化 pivot = pivot.fillna(0) pivot_tr = pivot.applymap(lambda x:1 if x&gt;1 else 0 if x == 1 else np.nan) pivot_tr.head() #求复购率，用sum统计购买次数大于1的用户数量，用count统计所有购买用户数 ratio = pivot_tr.sum()/pivot_tr.count() ratio #复购率： month 5 0.518338 6 0.540954 7 0.518946 8 0.516869 9 0.549189 10 0.565497 11 0.692119 dtype: float64 #可视化 plt.plot(ratio) plt.xlabel('month') plt.ylabel('repurchase rate') plt.suptitle('monthly repurchase rate') 分析小结： 各月份复购率均保持在50%以上，用户忠诚度高。 8—11月复购率持续增长： 原因假设：具有购买行为的用户数量下降，分母变小，导致复购率增加。 假设检验：分析购买用户数量。 收集证据： #查看各统计指标 pivot.describe() 整体上，每月具有购买行为的用户数量呈现上升趋势，其中5-10月增长比较平缓。 结论：假设不成立，复购率提升不是因为用户的流失。 （3）回购率 回购率分析思路：与复购率类似但更复杂一些，需要考虑下一个窗口期间的购买情况。 #生成透视表，统计每月购买情况 pivot_rep = user_buy.pivot_table(index = 'user_id',columns = 'month',values = 'action_type',aggfunc = 'count',fill_value = 0) #数据转换，将存在购买行为的情况用1表示，不存在的用0表示 pivot_rep_tr = pivot_rep.applymap(lambda x: 1 if x&gt;0 else 0) pivot_rep_tr.head() #定义函数转化透视表每行数值：本月与下月同时为1（购买）的，在本月生成1;本月与下月分布为1,0（本月有下月没有购买），在本月生成0；本月为0，则在本月生成nan. def transform(data): trans_list = [] for i in range(5,11): if data[i] == 1: if data[i + 1] == 1: trans_list.append(1) if data[i + 1] == 0: trans_list.append(0) else: trans_list.append(np.NaN) #因为11月没有后续月份，所以填入nan trans_list.append(np.NaN) return pd.Series(trans_list) #应用函数，apply表示使用函数转化前面的DataFrame,axis=1表示对象是整行 final_pivot = pivot_rep_tr.apply(transform,axis=1) #对列重命名 final_pivot.columns = range(5,12) final_pivot.head() #计算回购率 re_ratio = final_pivot.sum()/final_pivot.count() re_ratio #回购率： 5 0.513516 6 0.454410 7 0.463757 8 0.522471 9 0.565885 10 0.999968 11 NaN dtype: float64 #可视化 # 指定默认字体 plt.rcParams['font.sans-serif'] = ['KaiTi'] # 解决保存图像是负号'-'显示为方块的问 plt.rcParams['axes.unicode_minus'] = False 题 plt.plot(re_ratio) plt.xlabel('月份') plt.ylabel('回购率') plt.suptitle('7个月内回购率图') 分析小结： 回购率整体上分布在50%上下五个点的范围内，客户忠诚度高。 十月份的回购率达到了100%，说明了双十一的强大影响。 总结 从指标上看，留存率、复购率和回购率基本保持在50%以上，平台用户的忠诚度颇高，但同时也存在以下问题： 新用户增长缓慢、乏力。 购物狂欢节（618、双十一）可以有效拉动指标增长，但可能不具有持续性。 建议： 开拓新市场，尝试新的获客渠道；增强平台的社交属性，打造购买—分享—交友的生态圈。 可以用机器学习辨识预测哪些用户在节日过后更可能留存，有针对性地进行营销以降低成本。 Revenue：营收获取 假设1：路径越短，越有利于提高购买转化率。 假设2：特定的中间环节（加购、收藏）可以提高购买转化率。 验证思路：分析用户各路径的购买转化率，共有8条路径。 浏览—&gt;流失 浏览—&gt;购买 浏览—&gt;收藏—&gt;购买 浏览—&gt;收藏—&gt;流失 浏览—&gt;加入购物车—&gt;购买 浏览—&gt;加入购物车—&gt;流失 浏览—&gt;加入购物车、收藏—&gt;购买 浏览—&gt;加入购物车、收藏—&gt;流失 收集证据： 区分用户的不同行为路径 #去重 user_full_con = user_full.drop_duplicates(['user_id','item_id','action_type']) #将购买行为标记为10，以区分其他行为 user_full_con['action_type'] = user_full_con['action_type'].apply(lambda x: 10 if x == 2 else x) #按用户、商品分组对用户行为进行求和 group_con = user_full_con[['user_id','item_id', 'action_type']].groupby(['user_id','item_id']).sum() group_con.head() #浏览—&gt;流失标记为10 pv_buy = group_con[group_con['action_type'] == 10] #浏览—&gt;购买标记为0 pv_unbuy = group_con[group_con['action_type'] == 0] #浏览—&gt;收藏—&gt;购买标记为13 pv_fav_buy = group_con[group_con['action_type'] == 13] #浏览—&gt;收藏—&gt;流失标记为3 pv_fav_unbuy = group_con[group_con['action_type'] == 3] #浏览—&gt;加入购物车—&gt;购买标记为11 pv_cart_buy = group_con[group_con['action_type'] == 11] #浏览—&gt;加入购物车—&gt;流失标记为1 pv_cart_unbuy = group_con[group_con['action_type'] == 1] #浏览—&gt;加入购物车、收藏—&gt;购买标记为14 pv_fav_cart_buy = group_con[group_con['action_type'] == 14] #浏览—&gt;加入购物车、收藏—&gt;流失标记为4 pv_fav_cart_unbuy = group_con[group_con['action_type'] == 4] 计算不同路径的数量： list_action = [pv_buy,pv_unbuy,pv_fav_buy,pv_fav_unbuy, pv_cart_buy,pv_cart_unbuy,pv_fav_cart_buy,pv_fav_cart_unbuy] def act_num(data): act_list = [] for i in range(8): act = data[i].shape[0] act_list.append(act) return act_list pv_buy 484778 pv_unbuy 4789692 pv_fav_buy 55924 pv_fav_unbuy 477300 pv_cart_buy 2086 pv_cart_unbuy 9898 pv_fav_cart_buy 169 pv_fav_cart_unbuy 474 dtype: int64 结论：从总环节转化率来看，假设1成立；从单环节转化率来看，假设2成立 分析发现： 大部分用户（82%）在浏览后就会流失掉。 从总环节转化率来看，浏览—&gt;购买路径的转化率最高（8.329%）。 从单环节转化率来看，浏览—&gt;收藏、加购路径的转化率最高（26.3%），其次是浏览—&gt;加购路径（17.4%），同时，要注意的是，虽然浏览—&gt;收藏路径的转化率最低（10.5%），但其绝对人数（533224）远大于前两者。 建议： 针对用户浏览后迅速流失：调查流失原因。如果是推荐的商品不合适，可着手优化推荐系统；如果是网站引导有所欠缺，可进行相关的改善；如果是拉新渠道不对，获取的不是目标用户，可做相应调整。 浏览—&gt;购买路径的总环节转化率最高，说明路径越短，用户购买的可能性越高，因为更长的路径有可能冷却用户的购买热情。因此，可继续简化通向最终购买的环节。 可以使用触发物激活加购、收藏后不购买的用户，例如赠予优惠券等。 Kmeans模型：用户分层 分析思路与特征选取： 使用Kmeans算法，基于最近一次消费的时间间隔、购买频率、年龄和性别对用户进行分群。 最近一次消费的时间间隔：假设今天是2015年12月12日，最近一次消费的时间间隔就是今天与最近一次消费时间的差值。 购买频率：用户的购买次数。 年龄和性别 整理时间序列 #为方便求出最近一次消费的时间间隔，对数据的时间序列进行整理 #假设今天为2015年12月12日 now = dt.datetime(2015,12,12) #筛选出有购买行为的数据，同时剔除双十一的影响 user_buy = user_full[(user_full['action_type']==2) &amp; (user_full['month'] != 11)] user_buy.drop(['month','day','year'],axis=1,inplace=True) user_buy.head() 计算时间间隔： #求最近一次消费的时间间隔 user_buy['time_interval'] = now - user_buy['date'] #转换日期格式 user_buy['time_interval'] = user_buy['time_interval'] / np.timedelta64(1,'D') user_buy.head() 提取特征： #按用户分组，聚合取得最近购买距今天数、购买频率、年龄、性别 user_RFAG = user_buy.groupby('user_id').agg({'time_interval':np.min, 'user_id':np.size, 'age_range':np.max,'gender':np.average}) user_RFAG.rename(columns = {'time_interval':'recency', 'user_id':'frequency'},inplace = True) user_RFAG.head() 寻找最佳分组数量： from sklearn.cluster import KMeans #遍历k，找合适的k值，k表示将用户划分为k组 inertia = [] for i in range(1,15): model = KMeans(n_clusters=i,n_jobs=4) model.fit(user_RFAG) inertia.append(model.inertia_) plt.plot(range(1,15),inertia,marker='o') plt.xlabel('number of clusters') plt.ylabel('distortions') plt.show() 可见，k=3时，误差相对较低，应将客户划分为3组。 建立模型并分组： #建立模型并进行分群 k = 3 kmodel=KMeans(n_clusters=k,n_jobs=4) kmodel.fit(user_RFAG) #将结果合并 r1=pd.Series(kmodel.labels_).value_counts() r2=pd.DataFrame(kmodel.cluster_centers_) r3=pd.Series(['group1','group2','group3']) r=pd.concat([r3,r1,r2],axis=1) r.columns=['聚类类别','聚类个数']+list(user_RFAG.columns) r.sort_values(by='聚类个数',ascending = False).assign(ratio = r['聚类个数']/r['聚类个数'].sum()) 分析小结： group3购买间隔最短，频率最高，应划分为重要价值用户，可提供vip服务。 group1购买间隔、频率中等，应划分为一般价值用户，可通过触发物（例如推送告知特价商品、优惠活动等）提升其购买频率。 group2为普通用户，可与之保持联系，积极调查流失原因。 2.货 这部分将分析商品/商品种类的销量和销售关联性。 商品销量分析 （1）查看销售的总体情况 查看销量前十的商品： item_buy = user_buy[['item_id','action_type']].groupby('item_id').count().reset_index() item_buy = item_buy.rename(columns={'action_type':'buy_times'}) item_buy.sort_values(by = 'buy_times',ascending = False).head(10) 查看销量分布情况： item_buy_count = item_buy[['item_id','buy_times']].groupby('buy_times').count() item_buy_count = item_buy_count.rename(columns = {'item_id':'item_num'}) item_buy_count_head = item_buy_count.sort_values(by = 'item_num', ascending = False).head(10) item_buy_count_head.div(item_buy_count['item_num'].sum()) from matplotlib.pyplot import MultipleLocator plt.plot(item_buy_count_head) plt.xlabel('购买次数') plt.ylabel('商品数量') plt.suptitle('不同购买次数的商品数量分布') x_major_locator=MultipleLocator(1) ax=plt.gca() ax.xaxis.set_major_locator(x_major_locator) 分析小结： 销量最高的商品总共售出了846件，相比之下，接近80%的商品只出售过1到3次，呈现出长尾效应。 （2）分析商品销量的成因 假设：销量与流量、加购量、收藏量等因素相关 验证思路：查看每个商品对应的点击、购买、加购和收藏量，分析其中的联系。 收集证据： #筛选不同行为的数据 user_pv = user_full[user_full['action_type']==0] user_cart = user_full[user_full['action_type']==1] user_fav = user_full[user_full['action_type']==3] #按商品分组，查看各商品不同行为的数量 item_pv = user_pv[['item_id','action_type']].groupby('item_id').count() item_pv = item_pv.rename(columns = {'action_type':'pv_num'}) item_cart = user_cart[['item_id','action_type']].groupby('item_id').count() item_cart = item_cart.rename(columns = {'action_type':'cart_num'}) item_fav= user_fav[['item_id','action_type']].groupby('item_id').count() item_fav = item_fav.rename(columns = {'action_type':'fav_num'}) item_buy = item_buy.set_index('item_id') #将各行为数量合并 item = pd.merge(item_buy,item_pv,on = 'item_id',how = 'inner') item = pd.merge(item,item_cart,on = 'item_id',how = 'inner') item = pd.merge(item,item_fav,on = 'item_id',how = 'inner') #查看销量前十的商品 item.sort_values(by = 'buy_num', ascending = False).head(10) #查看浏览量前十的商品 item.sort_values(by = 'pv_num', ascending = False).head(10) #查看加购量前十的商品 item.sort_values(by = 'cart_num', ascending = False).head(10) #查看收藏量前十的商品 item.sort_values(by = 'fav_num', ascending = False).head(10) 结论： 流量前十的商品中只有一件位于销量前十，可能说明：1）用户本身需求并不多，但平台推荐多，会导致点击率高，但最终销量低 。2）用户本身有需求，而平台并没有推荐到位，用户通过自己搜索和寻找，促使该商品销量高。 加购和收藏次数前十的商品都只有两件位于销量前十，说明用户有购买的想法却因为种种原因没有购买； 建议： 改善推荐系统，推荐用户需要的商品，提高流量的转化率； 调查加购/收藏用户的流失原因，改善各环节的流畅度（例如支付环节是否太过繁琐）； 商品种类分析 分析商品各种类的销量： user_buy_cat = user_buy[['cat_id','action_type']].groupby('cat_id').count() user_buy_cat = user_buy_cat.rename(columns = {'action_type':'buy_count'}) user_buy_cat = user_buy_cat.sort_values(by = 'buy_count',ascending = False) user_buy_cat = user_buy_cat.reset_index() user_buy_cat.head() 可见，销量前五的商品种类差距并不大。 分析商品种类与用户行为的联系： #将具有购买行为的用户id和商品种类与全部数据合并，突显被购买商品种类对应的全部用户行为 user_behav_cat = pd.merge(user_buy[['user_id','cat_id']],user_full, on = ['user_id','cat_id'],how = 'left') #计算被购买商品种类对应的全部行为数 user_behav_cat = user_behav_cat[['cat_id','action_type']].groupby('cat_id').count() user_behav_cat = user_behav_cat.rename(columns ={'action_type':'behav_count'} ) user_behav_cat.sort_values(by = 'behav_count',ascending=False).head() #合并被购买商品种类的购买计数与行为计数 user_cat_buy_behav = pd.merge(user_buy_cat,user_behav_cat,on = 'cat_id',how = 'inner') user_cat_buy_behav = user_cat_buy_behav.reset_index() user_cat_buy_behav = user_cat_buy_behav.assign(ratio = user_cat_buy_behav['behav_count']/ user_cat_buy_behav['buy_count']) user_cat_buy_behav.head(10) #尾部商品种类 user_cat_buy_behav.tail(10) 分析小结： 头部商品种类销量大，但对应的行为—&gt;购买转化率较尾部种类低，平均20个行为转化为1个购买，而尾部商品种类的平均转化率为3：1 原因假设： 头部商品种类同质化较严重，竞争激烈，用户购买前需要更多的对比；而尾部商品种类则更具个性化，能更好地满足细分市场的需求。（因为数据不足无法继续论证） 建议： 尝试将热门商品种类与冷门商品种类捆绑销售，同时利用前者的高流量和后者的高转化。 注重长尾效应，积极开拓细分市场。 关联性分析 （1）分析各商品种类间的关联性 在购买行为中，按用户id和日期分组，生成订单 #计算总订单数 #假设一个用户一天内所有的购买都在一个订单 order_num = user_buy.groupby(['user_id','date']).count() order_num.shape[0] #总订单数 327130 #筛选出商品数量大于1的订单 order_items_num = order_num[order_num['item_id'] &gt; 1].reset_index() order_items_num = order_items_num[['user_id','date','item_id']].rename(columns={'item_id':'item_num'}) order_items_num.sort_values(by = 'item_num',ascending = False).head(10) 按销量排名前十的订单均发生在购物狂欢节。 在订单中添加商品id和种类： #合并，突显订单中的商品和商品种类 order_items_cat = pd.merge(order_items_num, user_buy, on = ['user_id','date'], how = 'inner') order_items_cat.head() 显示两两同时出现的商品种类： relation_cat_a = order_items_cat[['user_id','date','cat_id']] relation_cat_b = order_items_cat[['user_id','date','cat_id']] #自连接 relation = pd.merge(relation_cat_a,relation_cat_b, on = ['user_id','date'], how = 'left') #去重 relation = relation[relation['cat_id_x'] &lt; relation['cat_id_y']] relation.head() 通过计算种类两两出现的频率来表示种类之间的关联性： rel_count = relation[['cat_id_x','cat_id_y','user_id']].groupby(['cat_id_x','cat_id_y']).count() rel_count = rel_count.rename(columns = {'user_id':'relation'}).sort_values(by = 'relation',ascending = False) rel_count.reset_index().head(10) 分析小结： 某些商品种类之间存在关联性，用户购买了其中一种时可能也会购买另一种。 1213与420表现出与多个商品种类的强关联性 建议： 可根据商品种类间的关联性进行商品推荐 （2）分析各单独商品间的关联性 与种类关联性分析的方法基本一致 relation_item_a = order_items_cat[['user_id','date','item_id']] relation_item_b = order_items_cat[['user_id','date','item_id']] relation_item = pd.merge(relation_item_a,relation_item_b, on = ['user_id','date'], how = 'left') relation_item = relation_item[relation_item['item_id_x'] &lt; relation_item['item_id_y']] relation_item.head() rel_item_count = relation_item[['item_id_x','item_id_y','user_id']].groupby(['item_id_x','item_id_y']).count() rel_item_count = rel_item_count.rename(columns = {'user_id':'relation'}).sort_values(by = 'relation',ascending = False) rel_item_count.reset_index().head() 分析小结： 282032号和642180号商品具有最高的关联性。 1029992号商品和590204号商品表现出与多个商品的关联性。 3.场 卖家分析 分析各卖家流量与销量的联系 将用户对卖家商品的所有行为都视为卖家的流量。 #按卖家分组，统计商品数量 seller_group = user_full[['seller_id','item_id']].groupby('seller_id').count().reset_index() seller_group = seller_group.rename(columns = {'item_id':'item_count'}) seller_group.sort_values(by = 'item_count',ascending = False).head(10) #统计销售商品数量 seller_buy_group = user_buy[['seller_id','item_id']].groupby('seller_id').count().reset_index() seller_buy_group = seller_buy_group.rename(columns = {'item_id':'item_buy_count'}) #合并对比，算出转化率 seller_combined = pd.merge(seller_group,seller_buy_group,on = 'seller_id',how = 'left') seller_combined = seller_combined.assign(ratio = seller_combined['item_buy_count']/seller_combined['item_count']) seller_combined.sort_values(by = 'item_buy_count',ascending = False).head(10) #计算平均转化率 seller_combined['ratio'].mean() 0.0781 分析总结： 销量前十的卖家有六家同时也是流量前十的卖家，说明流量与销量正相关; 销量前十的高流量卖家转化率普遍较低，低于7.8%的平均水平，应该关注这些卖家的获客成本，分析其推广、营销费用是否过多，获客渠道是否合适等; 在销量前十的卖家中，1200号卖家以最低的流量获取了较高的销量，转化率一枝独秀，高达26.5%，应进一步研究: 原因假设：客户忠诚度高 验证思路：比较1200号卖家和销量第一的3828号卖家的复购率。 收集证据： #生成透视表，行索引为用户id,列索引为月份 seller_1200 = user_full[user_full['seller_id']==1200] seller_1200_buy = seller_1200[seller_1200['action_type']==2] pivot_1200_re = seller_1200_buy.pivot_table(columns='month',index='user_id', values='action_type',aggfunc='count') pivot_1200_re.head() #转换数值以计算复购率 pivot_1200_re = pivot_1200_re.applymap(lambda x:1 if x&gt;1 else 0 if x == 1 else np.nan) pivot_1200_re.head() #卖家1200复购率 ratio_1200 = pd.Series(pivot_1200_re.sum()/pivot_1200_re.count(),name = 're_ratio') #计算卖家3828复购率 seller_3828 = user_full[user_full['seller_id']==3828] seller_3828_buy = seller_3828[seller_3828['action_type']==2] pivot_3828_re = seller_3828_buy.pivot_table(columns='month',index='user_id',values='action_type',aggfunc='count') pivot_3828_re = pivot_3828_re.applymap(lambda x:1 if x&gt;1 else 0 if x == 1 else np.nan) ratio_3828 = pd.Series(pivot_3828_re.sum()/pivot_3828_re.count(),name = 're_ratio') #合并 ration_merge = pd.merge(ratio_1200,ratio_3828,on = 'month',how = 'inner') ration_merge = ration_merge.rename(columns = {'re_ratio_x':'re_ratio_1200','re_ratio_y':'re_ratio_3828'}) ration_merge.applymap(lambda x: format(x,'.2%')) 结论： 假设成立，1200号卖家的复购率整体上高于3828号卖家，如果其他条件不变，1200号卖家的顾客忠诚度更高。 六、增长建议 1.获取新用户方面： 积极探索开发主流人群（25-35岁女性）之外群体的商机，尝试比较不同的获客渠道，平衡成本与收益； 2.提高用户的体验与粘性： 探寻增强平台的社交属性，打造购买—分享—交友的生态圈； 可以通过机器学习、数据挖掘预测哪些用户在节日过后更可能留存，有针对性地进行营销； 优化推荐系统辅助用户决策； 3.促进用户交易： 简化通向购买行为的环节； 可以使用触发物激活加购、收藏后不购买的用户，例如赠予优惠券等； 积极向用户推荐关联性高的商品，促进连带销售； 尝试将热门商品与冷门商品捆绑销售，同时利用前者的高流量和后者的高转化； 4.用户画像/精准运营 针对用户的不同属性进行分层管理； ","link":"https://stdasein.life/post/dian-shang-zhi-biao-fen-xi/"}]}