{"posts":[{"title":"Lending Club贷款分析及违约预测","content":"一、项目背景 背景介绍 作为旧金山的一家个人对个人的借贷公司，Lending Club利用网络技术打造交易平台，直接连接了个人投资者和个人借贷者，通过此种方式，缩短了资金流通的细节，尤其是绕过了传统的大银行等金融机构，使得投资者和借贷者都能得到更多实惠、更快捷。对于投资者来说可以获得更好的回报，而对于借贷者来说，则可以获得相对较低的贷款利率。 数据来源 Kaggle:All Lending Club loan data 二、提出问题 通过探索性数据分析建立用户画像 利用机器学习进行违约预测 三、分析框架 四、前期准备 设置基本环境，导入pandas、numpy等要用到的库 import pandas as pd import numpy as np import matplotlib.pyplot as plt # 忽略弹出的warnings import warnings warnings.filterwarnings('ignore') # 指定默认字体 plt.rcParams['font.sans-serif'] = ['SimHei'] # 解决保存图像是负号'-'显示为方块的问题 plt.rcParams['axes.unicode_minus'] = False 探索文件体积大小，以防后续出现加载过慢或内存报错等问题 count = 0 fp = open('lendingclub.csv', &quot;r&quot;, encoding='utf-8') while 1: buffer = fp.read(8*1024*1024) if not buffer: break count += buffer.count('\\n') print(count) print('over') fp.close() 2260702 over 文件有200多万行，这可能导致内存不足，因此我们选择通过chunksize参数选取部分内容进行分析，chunksize的具体用法参见Stack Overflow path = 'lendingclub.csv' reader = pd.read_csv(path,chunksize = 500000) for i,ck in enumerate(reader): print(i,'',len(ck)) ck.to_csv('D:\\学习\\jupyter notebook'+str(i)+'.csv', index=False) 0 500000 1 500000 2 500000 3 500000 4 260701 我们将文件分成了5块，选择其中行数为260,701的1块进行分析 path = 'D:\\学习\\jupyter notebook4.csv' data = pd.read_csv(path) 五、探索性数据分析（EDA) 探索性数据分析（EDA)是指通过作图、制表、计算特征量等手段探索数据的结构和规律的一种数据分析方法，旨在为后续的流程提供决策辅助，其分析框架如下图所示: 1.概况分析 在这个环节我们的关注点集中在数据集的整体概况上，要求从特征值数量、数据质量、数据类型等方面对数据集进行评估。 查看数据集的样本数量和每个样本的特征数量： data.shape (260701, 151) 可见，数据集有260,701名用户的贷款资料，且有151个分析维度（贷款金额、工作年限等） 查看前5个样本： data.head() 注意：这里只截取了部分特征。显然，有些特征（例如member_id）有较多的缺失值，可考虑在数据预处理环节进行进一步分析、清理。 进行描述性统计分析： data.describe(include = 'all') describe()函数可以帮助我们了解数据的集中趋势（均值、众数）、离散程度（极差、标准差）和分布状况（分位数）等，而对于类别型数据，则能够提供唯一值数量、频数最高的值等信息。以emp_title为例，我们可以得知在众多贷款者中，从事最多的职业是老师，这揭示了金融机构放贷的偏好。 查看数据类型： num_feature = data.select_dtypes(include = ['number']).columns num_feature Index(['member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'fico_range_low', ... 'deferral_term', 'hardship_amount', 'hardship_length', 'hardship_dpd', 'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'settlement_amount', 'settlement_percentage', 'settlement_term'], dtype='object', length=113) cat_feature = data.select_dtypes(include = ['object']).columns cat_feature Index(['id', 'term', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'initial_list_status', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d', 'application_type', 'verification_status_joint', 'sec_app_earliest_cr_line', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date', 'hardship_loan_status', 'disbursement_method', 'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date'], dtype='object') 经过初步筛选，我们发现数据集中有113个数值型特征，其余皆为分类型，然而有些特征的类型需要在后续进行调整，例如工作年限em_length可能归为数值型更合适。 2.用户画像 （1）用户基本信息 地域分布（颜色越深表示该区域客户数量越多） 分析发现：客户数量最多的前四个州分别是加州(35,048）、德州(22,007)、纽约(21,210)和弗罗里达(19,519)，而爱荷华客户数量最少(0)；此外，沿海地区的客户数量整体上要高于内陆地区。 提出假设：贷款数量与区域经济发展程度有关。 验证假设：我们可以将平均年收入和总年收入作为衡量一个州经济发展程度的指标，以此来验证先前提出的假设。 上图中，方块越大代表总收入越高，而颜色越红代表年均收入越高。 我们可以看见，经济规模前四的州分别是加州、德州、纽约和弗罗里达，正好与贷款数量前四的州一一对应；同时，年均收入较高的州（颜色红）大部分都位于沿海地区。 收入分布 分析发现：客户收入呈现正偏态分布（即平均数大于中位数，中位数大于众数），其中，位于4,0000-6,0000美元收入区间内的客户人数最多。 提出假设：相较于高收入人群，收入中等偏下的人群有更高的贷款需求，而低收入人群则更难通过贷款审核。 职业分布 分析发现：除了经理(manager)这一职业占比高达14.98%外，前十名的其他职业，例如主管(Supervisor 2.87%)、老师(2.86%)等，占比差距并不大。 提出假设：以经理为职业的人群收入更高，机构更容易发放贷款。 验证假设：我们可以尝试加入年收入中值维度进行分析（以颜色表示，从深绿到深黄，年收入中值递增） 结果显示，尽管经理的收入在前十的职业中并不低，但也没有明显的优势，不能简单的认为其数量优势是来自与高收入，因此假设不成立。 修改假设：经理可能是更具代表性的管理层的一种泛称，客户倾向于使用这样的称呼来代替具体的管理职务。 工作年限 根据经验，工作年限越长收入越高，因此我们可以将工作年限与收入进行组合分析来验证这一点。 分析发现：收入与工作年限成正比（由绿到红）；10年及以上工作年限的客户占比最高（36.08%），说明贷款机构偏好高工作年限的客户，但另一方面，低工作年限（0-3年）的人群占比也接近35%。 提出假设：贷款机构可能通过提高利率、降低贷款额来弥补风险（对低工作年限人群） 验证假设 总的来说，工作年限越高，贷款额度越高，同时，利率也越低（红到灰），假设成立。 住房情况 分析发现：约半数的客户有抵押贷款，经济实力较强、拥有自己的房子的客户仅占12.35%，而剩下的将近40%的客户处于租房住的状态。 提出假设：住房情况与工作年限、收入正相关。 验证假设：通过仪表板关联图表进行分析 对比发现，40%拥有住房的客户工作年限在10年以上，而租房客户仅占约26%，假设成立。 贷款目的 分析发现：大部分贷款目的（55%债务重组、20%还信用卡，总共75%）是借新债还旧债。 提出假设：以还债为目的的贷款利率更高。 验证假设：以利率和贷款目的为坐标轴画散点图 信用卡、债务重组项的利率并不突出，假设不成立。 月偿还额收入比 大部分客户的月偿还收入比都在30%以下，其中偿收比在10%-21%的人群占比约为55%，可见，整体上，客户群体的偿债能力还是比较高的。 （2）客户信用记录 借款者过去两年借款逾期超过30天次数 距离最近逾期的月份数 负面记录 违约金额 借款者过去两年借款逾期超过30天次数 气泡图显示，81.05%的客户在近两年不存在逾期行为，说明大部分客户还是信用良好的，而在有逾期行为的客户中，逾期1次的占多数（12.6%）。 距离最近一次逾期的月数&amp;负面记录分析 分析发现：如果把时间维度拉长，曾有逾期行为的客户数量将增至接近总数的一半（130,026），但是，机构对客户的负面记录基本与近两年逾期情况的分布一致，这可能说明在很大程度上，机构只考虑前两年的信用状况。 违约金额分布 分析发现：从整体上来说，逾期次数与逾期金额成反比，逾期金额主要分布在1100美元以下的区间内，其中0—200美元区间内的逾期次数最多。逾期的贷款多是用于借新债还旧债，与贷款目的分布一致。 （3）产品数据 借款者的信用等级 贷款额度 贷款期限 贷款利率 贷款状态 信用等级、贷款额度与利率（不同的形状代表不同的信用等级，颜色代表频率） 分析发现： 利率与信用等级成反比，信用等级越低则利率越高，其中A级信用的利率均值仅为7.03%，而G级却高达30.35%; B级和C级发放贷款的总量最高，G级最低。 贷款额度、利率与期限（颜色代表贷款额度） 分析发现：机构偏向发放低期限的贷款，尽管高期限的贷款利率更高，可能是出于规避风险的需要。 贷款状态 分析发现: 有53.19%的贷款还在进行中，无法判断最终是否形成坏账 有33.82%的贷款顺利完成； 有10.48%的借款无法收回（charged off） 有1.62%的客户存在逾期（30-120天） 六、数据预处理 1.处理缺失值 处理缺失值主要有5种方法： 删除含有缺失值的特征：适用于缺失值过多的特征 特殊值填充：这个是认为数据的空值也是具有一定的信息的，它之所以为空，是因为它不同于其他的任何数据。所以将空值作为一种特殊的属性值来处理，它不同于其他的任何属性值。如所有的空值都用“unknown”填充。 统计值填充：如果空值是数值型的，就根据该属性在其他所有对象的取值的统计值（例如平均值）来填充该缺失的属性值；如果空值是非数值型的，就根据统计学中的众数原理，用该属性在其他所有对象的取值次数最多的值(即出现频率最高的值)来补齐该缺失的属性值。 最近邻法：先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本，将这K个值加权平均来估计该样本的缺失数据。缺点：在于难以定义相似标准，主观因素较多。 模型预测：基于完整的数据集，建立预测模型。对于包含空值的对象，将已知属性值代入方程来估计未知属性值，以此估计值来进行填充。其实就是假设特征之间也存在一定的关系，可以通过预测来得到缺失值。但是我个人不建议使用这个方法，因为有些麻烦，而且不确定这样得到的填充值的效果。又可能出现模型过拟合等新问题。 为了方便起见，我们选择前三种方法处理缺失值。 （1）查看缺失值分布 #定义计算各特征缺失比例的函数 def CountNull(data): null_count = data.isnull().sum().sort_values(ascending = False) ratio = null_count/len(data) nulldata = pd.concat([null_count,ratio],axis = 1, keys=['count','ratio']) return nulldata[ratio&gt;0] CountNull(data) （2）使用dropna删除含缺失值的特征 通过查看缺失值的分布，我们发现部分特征的缺失情况很严重，有的缺失比例甚至能够达到99%以上，这样的特征对预测没有实际的意义，可以考虑设置阈值（例如缺失比达到50%），并用dropna删除达到阈值的特征。 #用dropna删去缺失值比例大于50%的特征 half_count = len(data)/2 data = data.dropna(thresh = half_count, axis = 1 ) data.shape (260701, 108) CountNull(data) 经过筛选，特征重由原来的151个变为108个。 （3）填充缺失值 填充数值型特征的缺失值 筛选空值有含义的特征，填充特殊值 对于其余特征，包括缺失值占比在1%以下的特征，直接用平均值进行填充。 #筛选数值型特征 num_feature = data.select_dtypes('number') num_names = num_feature.columns num_names Index(['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'annual_inc', 'loan_status', 'dti', 'delinq_2yrs', 'fico_range_low', 'fico_range_high', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries',......]) 根据官方的说明文件，'il_util','num_tl_120dpd_2m','bc_util'的空值等同于0，以此我们可以将其筛选出来。另外，'mths_since_recent_inq'表示距离最近一次逾期的月份数，缺失值可能是因为客户不存在逾期情况，我们可以选择填充一个较大的数字，例如100，将其与其他情况区分开来。 num_zero = ['il_util','num_tl_120dpd_2m','bc_util'] num_100 = ['mths_since_recent_inq'] data[num_zero] = data[num_zero].fillna(0) data[num_100] = data[num_100].fillna(100) 对余下的特征用均值进行填充 #筛选含有缺失值的数值型特征 num_null = CountNull(data).index nominal_null_feature = [i for i in num_null if i in num_names] #填充均值 data[nominal_null_feature] = data[nominal_null_feature].fillna(data[nominal_null_feature].mean()) 填充类别型特征的缺失值 直接填充'Unknown' cat_feature = data.select_dtypes('object') cat_names = cat_feature.columns cat_names Index(['id', 'term', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'verification_status', 'issue_d', 'pymnt_plan', 'url', 'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'initial_list_status', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d', 'application_type', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag'], dtype='object') data[cat_names] = data[cat_names].fillna('Unknown') data.isnull().sum() 2.处理异常值 （1）简单统计 des = data.describe() des.sort_values(by = 'std',ascending = False,axis = 1) 对标准差进行排序后，我们发现'tot_hi_cred_lim'（最高限额）的离散程度最大，这说明该特征可能存在异常值，需要我们作出进一步的判断。 我们可以利用散点图找出异常值，x变量为最高限额，y变量为年收入。 import seaborn as sns sns.scatterplot(x = 'tot_hi_cred_lim', y = 'annual_inc', data = data) 一般来说，信用额度与收入成正比，但右下角的点显然不符合这一规律，我们可以将其视为异常点进行删除 data[data['tot_hi_cred_lim'] &gt; 5000000] data.drop([101679,235281], axis = 0, inplace = True) （2）使用IsolationForest剔除异常值 IsolationForest是适用于连续数据的无监督异常检测方法，该算法会将数据只占很少量、数据特征值和正常数据差别很大的值视为异常值。 data.shape (260697, 108) #建立算法模型： from sklearn.ensemble import IsolationForest iso = IsolationForest() #剔除异常值：mask表示不含有异常值的行 yhat = iso.fit_predict(data[num_names]) mask = yhat != -1 data_iso = data.values[mask, :] data_iso = pd.DataFrame(data_iso,columns = data.columns) #转换数据类型 data_iso[num_names] = data_iso[num_names].astype('int') data_iso.shape (254811, 108) 可见，算法剔除了5,886个样本。让我们来看看剔除部分异常值后的效果： data_iso.describe().sort_values(by = 'std',axis = 1,ascending = False) 剔除了部分异常值后，数据的离散程度有所下降，但相对而言还是比较高，这可能是由贫富差距导致的，反映了现实的状况。 3.过滤冗余特征 冗余特征可以分为三类： 与预测无关的特征 无区分度的特征 信息重复的特征 （1）与预测无关的特征 没有实际意义：id; zip_code地址邮编；addr_state申请地址；url:网站链接; emp_title职位名称（在数据探索阶段进行了分析）；next_pymnt_d；last_pymnt_d；issue_d；earliest_cr_line；last_credit_pull_d 信息泄露：collection_recovery_fee; recoveries （2）无区分度的特征 data_iso.nunique() nuniq[nuniq == 1] policy_code 1 policy_code的值全为1，对预测没有影响，可以删去 （3）信息重复的特征 title： title与purpose的信息重复 sub_grade：与Grade的信息重复 通过drop将上述筛选出来的冗余特征删除： drop_features = ['emp_title','id','policy_code','zip_code','addr_state','url', 'next_pymnt_d','last_pymnt_d','collection_recovery_fee', 'last_pymnt_amnt','issue_d','title','sub_grade','last_credit_pull_d', 'earliest_cr_line'] data_iso.drop(drop_features,axis=1,inplace = True) 七、特征工程 什么是特征工程呢？一个非常简单的例子，现在出一非常简答的二分类问题题，请你使用逻辑回归，设计一个身材分类器。输入数据X:身高和体重 ，标签为Y:身材等级（胖，不胖）。显然，不能单纯的根据体重来判断一个人胖不胖，姚明很重，他胖吗？显然不是。针对这个问题，一个非常经典的特征工程是，**BMI指数，BMI=体重/(身高^2)。这样，通过BMI指数，**就能非常显然地帮助我们，刻画一个人身材如何。甚至，你可以抛弃原始的体重和身高数据。所以说，特征工程就是通过X，创造新的X'。基本的操作包括:衍生（升维），筛选（降维） 1.特征衍生 特征衍生就是通过现有特征来构建新特征，以提升机器学习的准确度。 根据原有特征，我们可以尝试构建以下新特征： fico信用评分均值：数据集给出了fico信用评分的上下限，我们可以以此构建fico评分的均值并删除原来的特征。 构建信用额度使用率：通过不同账户的信用余额/信用额度，可以获得每个账户的信用额度使用率，该特征可以衡量客户的资金使用情况。 构建总借款账户数量：将不同借款账户数量简单相加。 #fico评分均值 data_iso['fico_avg'] = (data_iso['fico_range_low']+data_iso['fico_range_high'])/2 #用户分期账户信用额度使用率 data_iso['installment_ratio'] = data_iso['total_bal_il']/ data_iso['total_il_high_credit_limit'] #循环贷款信用额度使用率 data_iso['credit_revolving_ratio'] = data_iso['revol_bal']/ data_iso['total_rev_hi_lim'] #总信用额度使用率 data_iso['total_ratio'] = data_iso['total_bal_ex_mort']/ data_iso['tot_hi_cred_lim'] #借款账户数 = 抵押贷款账户数+分期付款账户数+循环账户数 data_iso['num_accounts'] = data_iso['mort_acc'] + data_iso['num_il_tl'] + data_iso['num_rev_accts'] 2.特征规整 （1）数值型特征规整 特征缩放 在运用一些机器学习算法的时候不可避免地要对数据进行特征缩放（feature scaling），比如：在随机梯度下降（stochastic gradient descent）算法中，特征缩放有时能提高算法的收敛速度。特征缩放还可以使机器学习算法工作的更好。比如在K近邻算法中，分类器主要是计算两点之间的欧几里得距离，如果一个特征比其它的特征有更大的范围值，那么距离将会被这个特征值所主导。 数据整理、筛选： #将'loan_status'转换为0和1的形式，不存在违约行为（Current、Fully Paid）用0表示，其余用1表示 map_loan_status = {'Current':0,'Fully Paid':0,'In Grace Period':1,'Late (31-120 days)':1,'Late (16-30 days)':1,'Charged Off':1} data_iso['loan_status']= data_iso['loan_status'].map(map_loan_status) data_iso['loan_status'] = data_iso['loan_status'].astype('object') #筛选数值型特征 num_features = data_iso.select_dtypes('number') num_names = num_features.columns #因为构建的新特征可能重新引入无穷值和缺失值，所以要再次进行处理 data_iso.replace([np.inf, -np.inf], np.nan, inplace=True) data_iso.fillna(0, inplace=True) 使用sklearn工具进行缩放： #引入标准化工具并进行标准化 from sklearn.preprocessing import MinMaxScaler #创建对象，限定范围（1，2） scaler = MinMaxScaler(feature_range=(1, 2)) data_iso[num_names] = scaler.fit_transform(data_iso[num_names]) data_iso[num_names].head() 缩放后，数值的大小被控制在1到2之间。 （2）类别型特征规整 cat_features = data_iso.select_dtypes('object') cat_names = cat_features.columns cat_names Index(['term','grade','emp_length','home_ownership', 'verification_status', 'pymnt_plan', 'purpose', 'initial_list_status', 'application_type', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag'], dtype='object') 有序变量 数据特征中存在一些顺序变量(ordinal variable),它们不同于一般的类型变量（categorical variable），顺序变量之间存在固有的顺序 比如 (低, 中, 高)。 grade评级 (信用风险:A&lt;B&lt;C&lt;D&lt;E&lt;F&lt;G) emp_length工作年限 对有序变量，我们可以采用手动编码来体现顺序关系： #对grade进行编码 grade_dict = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'Unknown':8} data_iso['grade'] = data_iso['grade'].map(grade_dict) #对emp_length进行编码 emp_length_dict = {&quot;10+ years&quot;: 10, &quot;9 years&quot;: 9, &quot;8 years&quot;: 8, &quot;7 years&quot;: 7, &quot;6 years&quot;: 6, &quot;5 years&quot;: 5, &quot;4 years&quot;: 4, &quot;3 years&quot;: 3, &quot;2 years&quot;: 2, &quot;1 year&quot;: 1, &quot;&lt; 1 year&quot;: 0, &quot;Unknown&quot;: 0 } data_iso['emp_length'] = data_iso['emp_length'].map(emp_length_dict) data_iso[['grade','emp_length']] 编码后，特征值变为有顺序的数值。 无序变量 对于无序变量，我们可以使用pandas自带的get_dummies来进行独热编码。所谓独热编码，指的是用0和1来表示一个特征值，例如，颜色特征里面有两个特征值（蓝和绿），进行独热编码后，蓝就表示为（1，0），而绿就是（0，1），具体使用见官方文档 #用get_dummies对剩下的类别型特征进行独热编码 data_iso = pd.get_dummies(data_iso) data_iso.head() 截取了部分特征转换后的样子，可以看见，application_type被扩展成了3个特征。 3.特征选择 通过特征选择减少特征具有重要的现实意义，不仅减少过拟合、减少特征数量（降维）、提高模型泛化能力，而且还可以使模型获得更好的解释性，增强对特征和特征值之间的理解，加快模型的训练速度，一般的，还会获得更好的性能。 特征选择的方法主要有三种：过滤法、嵌入法、包装法 过滤法：完全独立于任何机器学习算法。它是根据各种统计检验中的分数以及相关性的各项指标来选择特征，适合在数据量很大的时候使用。 嵌入法：相比于过滤法**，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果**。并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版，因此可以完全不过滤，直接使用嵌入法。 包装法：区别于过滤法和嵌入法的一次训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。 考虑到数据量比较大和算力有限，嵌入法结合过滤法是比较合适的选择。 （1）嵌入法 #分离特征和预测标签 X = data_iso.drop(['loan_status'],axis = 1) y = data_iso['loan_status'] 找到阈值（阈值决定要保留多少特征，阈值越高，保留的特征越少） #通过学习曲线探索阈值 from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier as RFC from sklearn.model_selection import cross_val_score RFC_ = RFC(n_estimators = 10,random_state = 42) threshold = np.linspace(0,(RFC_.fit(X,y).feature_importances_).max(),20) score = [] for i in threshold: X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y) once = cross_val_score(RFC_,X_embedded,y,cv=5).mean() score.append(once) plt.plot(threshold,score) plt.show() 学习曲线显示，当阈值在0-0.02附近时，模型的评分最高，我们可以进一步细化学习曲线找到具体的值： #细化学习曲线 score2 = [] for i in np.linspace(0,0.02,20): X_embedded = SelectFromModel(RFC_, threshold=i).fit_transform(X,y) once = cross_val_score(RFC_, X_embedded, y, cv=5).mean() score2.append(once) plt.figure(figsize=[20,5]) plt.plot(np.linspace(0,0.02,20),score2) plt.xticks(np.linspace(0,0.02,20)) plt.show() 可见，当阈值约为0.0043时，模型评分最高。 特征选择 sfm = SelectFromModel(RFC_,threshold=0.0043) X_em = sfm.fit(X,y) #get_support()返回被选择的特征 features_keeped = X.columns[X_em.get_support()] X = X[features] X.shape (254723, 18) 经过嵌入法的筛选，特征值只剩下最重要的18个。 查看特征重要性排序 识别特征重要性对业务有指导作用，重要性越高，说明特征对业务目标的影响越大，意味着在决策的时候应予以更多的考虑。 RFC_.fit(X,y) importances = RFC_.feature_importances_ names = X.columns #返回按importances的值从大到小的排序，[::-1]表示倒序 indices = np.argsort(importances)[::-1] #可视化 fig = plt.figure() plt.title('Feature importances') plt.bar(range(X.shape[1]), importances[indices]) plt.xticks(range(X.shape[1]), names[indices],rotation='vertical',fontsize=14) plt.xlim([-1, X.shape[1]]) plt.show() 按重要性排序前5的特征分别是： last_fico_range_low last_fico_avg last_fico_range_high last_pymnt_amnt out_prncp 其中，last_fico_avg是根据last_fico_range_high和last_fico_range_low计算出来的均值，说明特征衍生是有效果的，但同时也可能造成冗余（特征之间高度相关，即共线性），因此，我们可以继续使用过滤法消除这种冗余。 （2）过滤法 使用pearson相关性分析，找出并剔除冗余特征： corr = X.corr() colormap = plt.cm.plasma plt.figure(figsize=(15,15)) plt.title('Pearson Correlation', y=1.05, size=15) sns.heatmap(corr,linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True) 删除相关性在0.85以上的部分特征： loan_amnt：funded_amnt、funded_amnt_inv、installment out_prncp：out_prncp_inv total_pymnt：total_pymnt_inv、total_rec_prncp last_fico_avg：last_fico_range_high、last_fico_range_low tot_cur_bal：tot_hi_cred_lim drop_list = ['funded_amnt','funded_amnt_inv','installment', 'out_prncp_inv','total_pymnt_inv', 'total_rec_prncp','last_fico_range_high', 'last_fico_range_low'] X = X.drop(drop_list,axis = 1) X.shape (254723, 10) 经过过滤，特征由18个下降为10个。 X.columns Index(['loan_amnt', 'out_prncp', 'total_pymnt', 'total_rec_int', 'total_rec_late_fee', 'last_pymnt_amnt', 'fico_avg', 'last_fico_avg', 'debt_settlement_flag_N', 'debt_settlement_flag_Y'], dtype='object') 八、模型训练 1.处理样本不均衡 周志华《机器学习》中介绍到，分类学习方法都有一个共同的基本假设，即不同类别的训练样例数目相当。如果不同类别的训练样例数目稍有差别，对学习结果的影响通常也不大，但若样本类别数目差别很大，属于极端不均衡，则会对学习过程（模型训练）造成困扰。这些学习算法的设计背后隐含的优化目标是数据集上的分类准确度，而这会导致学习算法在不平衡数据上更偏向于含更多样本的多数类。 查看不同类别的样本数目 from collections import Counter counter = Counter(y) print(counter) Counter({0: 220384, 1: 32107}) 违约样本数量为32,107，大约占总样本的1/8，样本分布不均衡。 SMOTE 和 RandomUnderSampler SMOTE是一种过采样方法：通过增加少数类别样本数目来达到样本均衡的效果 RandomUnderSampler一种欠采样方法：通过减少多数类别样本的数目达到样本均衡 两种方法一起使用可以有效处理样本不均衡的情况，从而提高预测的准确度。 from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler from imblearn.pipeline import Pipeline #用SMOTE提升少数类别样本数目，使之达到多数样本数目的30% over = SMOTE(sampling_strategy=0.3) #用RandomUnderSampler降低多数类别样本数目，使之比少数样本数目多40% under = RandomUnderSampler(sampling_strategy=0.4) #用pineline进行整合 steps = [('o', over), ('u', under)] pipeline = Pipeline(steps=steps) # 转换 X_resample, y_resample = pipeline.fit_resample(X, y) counter = Counter(y_resample) print(counter) Counter({0: 165287, 1: 66115}) 经过均衡处理，现在的样本比例约为2.5：1 评估均衡处理的效果 from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from numpy import mean model = DecisionTreeClassifier(random_state = 42) 定义评估函数，注意，这里使用的评分指标是roc_auc，该指标适用于评估样本失衡情况下的模型表现 def evaluate(data_X, data_y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42) scores = cross_val_score(model,data_X,data_y,scoring='roc_auc',cv=cv,n_jobs=-1) print('Mean ROC AUC:%.3f'%(mean(scores))) 评估均衡处理前后的模型表现 evaluate(X,y) Mean ROC AUC:0.934 evaluate(X_resample,y_resample) Mean ROC AUC:0.957 模型评分挺高了0.023 2.模型评估 引进常用的分类模型 简单线性模型：LogisticRegression 向量机：LinearSVC 最邻近算法：KNeighborsClassifier 决策树模型：DecisionTreeClassifier 聚合模型：RandomForestClassifier、AdaBoostClassifier、XGBClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import LinearSVC from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from xgboost import XGBClassifier def get_models(): models = dict() models['LR'] = LogisticRegression() models['SVC'] = LinearSVC() models['KNC'] = KNeighborsClassifier() models['DTC'] = DecisionTreeClassifier() models['forest'] = RandomForestClassifier() models['ABC'] = AdaBoostClassifier() models['XGB'] = XGBClassifier() return models models = get_models() 模型评分 def evaluate_model(model): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42) scores = cross_val_score(model,X_resample,y_resample, scoring='roc_auc',cv=cv,n_jobs=-1) return scores results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model) results.append(scores) names.append(name) print('&gt;%s %.3f'%(name,mean(scores))) &gt;LR 0.980 &gt;SVC 0.981 &gt;KNC 0.988 &gt;DTC 0.957 &gt;forest 0.994 &gt;ABC 0.980 &gt;XGB 0.985 评分最高的是随机森林模型（0.994），最低的是决策树（0.957），但考虑到在特征选择阶段我们曾用随机森林模型进行筛选，样本存在偏向，所以不能简单地认为随机森林就一定会比其他模型表现好，对此，我们可以使用堆叠法综合各模型的优点。 3.堆叠法（Stacking) Stacking是常见的集成学习框架。一般来说，就是训练一个多层(一般是两层，本文中默认两层)的学习器结构，第一层(也叫学习层)用n个不同的分类器(或者参数不同的模型)将得到预测结果合并为新的特征集，并作为下一层分类器的输入。一个简单的示意图如下： Stacking可以对多个单模型进行融合以提升整体性能，是在各种比赛中很常用的一种集成方法，详情参见详解stacking过程 定义Stacking模型 Stacking模型一般为两层结构，第一层使用性能比较好的模型，例如XGB、随机森林等，第二层则一般使用简单的模型，例如Logistic,以防止过拟合。 from sklearn.ensemble import StackingClassifier def get_stacking(): level0 = list() level0.append(('SVC',LinearSVC())) level0.append(('KNC',KNeighborsClassifier())) level0.append(('forest',RandomForestClassifier())) level0.append(('ABC',AdaBoostClassifier())) level0.append(('XGB',XGBClassifier())) level1 = LogisticRegression() model = StackingClassifier(estimators = level0,final_estimator = level1, cv = 10) return model model_stack = get_stacking() 模型评估 score = evaluate_model(model_stack) print('&gt;stacking %.3f'%mean(score)) &gt;stacking 0.993 最后的Stacking模型评分为0.993，虽然略低于随机森林，但因为集成了多个模型的性能，所以稳定性应该要比随机森林要好。 ","link":"https://stdasein.life/post/Lending Club Loan/"},{"title":"房价预测","content":"项目背景 房价预测是kaggle平台上的一个线性回归问题，要求参赛者根据房屋的属性预测房价。 数据概况 数据来源：kaggle 数据内容：数据集含有79个特征（房子的各种属性：面积、有无游泳池、卧室数量等）以及1个标签（房价） 数据详情 框架 数据探索：对数据集进行初步观察以获得整体概念。 数据清理：对数据集可能存在的缺失值和异常值进行处理。 特征工程：在理解现有特征的基础上尝试构建新特征，并进行规整、选择。 模型构建：构建预测模型、对模型评分并选择最优的模型进行预测。 数据探索分析 本阶段的目的是回答以下问题： 1、特征的数据类型是什么？ 2、特征都有什么特点？ 3、哪些特征还不能直接使用，需要做进一步的处理？ 引进常用的库来满足数据分析和可视化的需求： #基本环境设置 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas_profiling as pp #加载训练集和测试集 path1 = 'train.csv' path2 = 'test.csv' train_data = pd.read_csv(path1) test_data = pd.read_csv(path2) train_data.head() test_data.head() 为了方便操作，分离特征与标签并将训练集与测试集合并： #分离特征和标签 X_train = train_data.drop(['SalePrice'],axis=1) y_train = train_data['SalePrice'] #合并 X_data = X_train.append(test_data) #id列对预测没有意义，可舍去 X_data.drop(['Id'],axis=1,inplace=True) 使用pandas_profiling进行数据探索，对于数据集的每一列，pandas_profiling会提供以下统计信息： 1、概要：数据类型，唯一值，缺失值，内存大小 2、分位数统计：最小值、最大值、中位数、Q1、Q3、最大值，值域，四分位 3、描述性统计：均值、众数、标准差、绝对中位差、变异系数、峰值、偏度系数 4、最频繁出现的值，直方图/柱状图 5、相关性分析可视化：突出强相关的变量，Spearman, Pearson矩阵相关性色阶图 report = pp.ProfileReport(X_data) report Overview Overview部分提供了数据集的整体信息：缺失值数量、重复行数和数据的类型。 可见，数据集没有重复的行或列，但有13,965个缺失值需要处理，且数据主要为文本型（46）和数值型（33），这在后续分析时要我们进一步区分。 Variables variables部分会显示各特征的详细情况，例如、缺失值数量、唯一值数量、均值、标准差等。 以LotArea特征为例，可知其数据类型为数值型，没有缺失值，但标准差相较于其他特征要大得多，意味着可能存在异常值。 Correlations Correlations衡量各变量之间的相关性，为特征工程提供支持。 数据清理 错误的数据会使模型的预测发生偏差，所以在开始预测之前要先进行数据清理，处理数据中的噪音。 数据清理可分为三个步骤：处理缺失值、处理异常值和处理重复值。我们在数据探索部分已经知道数据集不存在重复，所以这里只需关注前两种情况。 缺失值处理 缺失值处理主要有三种方法： 1、dropna：将含有缺失值的整列/行舍弃； 2、fillna：用该列/行的均值、中位数或众数等填充缺失值； 3、模型预测：可用knn等模型预测缺失值； 模型预测方法相对来说会有更高的可靠性，但由于本数据集的数据缺失大部分都有单一且明确的原因，所以用fillna的方法会更加方便灵活。 #查看缺失值占比 def CountNull(data): null_count = data.isnull().sum().sort_values(ascending = False) ratio = null_count/len(data) nulldata = pd.concat([null_count,ratio],axis = 1, keys=['count','ratio']) return nulldata[ratio&gt;0] CountNull(X_data) 根据官方提供的data_description文本，有些特征出现缺失值是因为房子没有该特征（例如有的房子没有游泳池），对于这一类可以选择填入None，如果是数值类型的就填入0；除此之外，对于缺失值比较少的特征（缺失值仅有1或2个），可以直接填充众数。 #选择缺失值数量为1或2的特征 feature_fill_mode = CountNull(X_data)[(CountNull(X_data)['count'] == 1) | (CountNull(X_data)['count'] == 2)].index #因为同一个社区的LotFrontage（街道到房屋的距离）应该比较接近，所以填充同社区的中位数 feature_fill_neig = ['LotFrontage'] #选择要填入none的特征 feature_fill_none = ['MasVnrType','BsmtFinType1','BsmtFinType2','BsmtQual','BsmtExposure','BsmtCond', 'GarageType','GarageFinish','GarageQual','GarageCond','FireplaceQu','Fence','Alley', 'MiscFeature','PoolQC','MSZoning'] #选择要填入0的特征 feature_fill_zero = ['MasVnrArea','GarageYrBlt'] 对特征的缺失值进行填充： for col in feature_fill_mode: X_data[col].fillna(X_data[col].mode()[0], inplace=True) for col in feature_fill_none: X_data[col].fillna('None',inplace=True) for col in feature_fill_zero: X_data[col].fillna(0,inplace=True) X_data['LotFrontage'] = X_data.groupby(&quot;Neighborhood&quot;)['LotFrontage'].transform(lambda x: x.fillna(x.median())) CountNull(X_data) 所有缺失值均已填充完毕。 异常值处理 （1）观察法 所谓观察法，就是通过描述性统计分析和可视化来寻找可能存在的异常值。 #筛选数值型特征 num_features = X_data.select_dtypes('number') num_names = num_features.columns des = X_data[num_names].describe() #按特征的标准差排序 des.sort_values(by = 'std',ascending = False, axis = 1) 可见，LotArea的标准差要远大于其他特征，且极差也非常大，应进一步研究是否存在异常值。 ax = sns.scatterplot(x=&quot;LotArea&quot;, y=&quot;SalePrice&quot;, data=train_data) #用中位数替换异常值 X_data.loc[X_data['LotArea']&gt;150000,['LotArea']]=9453 （2）使用IsolationForest剔除异常值 IsolationForest是适用于连续数据的无监督异常检测方法，该算法会将数据只占很少量、数据特征值和正常数据差别很大的值视为异常值。 #因为只需剔除训练集的异常值，所以暂时将训练、测试集分离 X_train = X_data.iloc[:1460,:] X_test = X_data.iloc[1460:,:] X_train.shape (1460, 79) #建立算法模型： from sklearn.ensemble import IsolationForest iso = IsolationForest() #剔除异常值：算法将-1表示为异常值，所以mask表示不含有异常值的行 yhat = iso.fit_predict(X_train[num_names]) mask = yhat != -1 X_train, y_train = X_train.values[mask, :], y_train.values[mask] X_train = pd.DataFrame(X_train,columns = X_data.columns) X_train.shape (1380, 79) 算法剔除了80行包含有异常值的数据。 #将训练集和测试集合并 X_data_removed = pd.concat([X_train,X_test],axis = 0) X_data_removed[num_names] = X_data_removed[num_names].astype(int) X_data_removed.reset_index(drop=True, inplace=True) 特征工程 特征工程是机器学习的核心环节，好的特征工程可以有效提高预测的精度，其主要步骤如下图所示： 特征类型 因为不同的数据类型有不同的处理方式，所以第一步是识别各特征的数据类型： 1、数值型：连续、离散 2、类别型 #筛选全部数值型特征 numeric_feature = X_data_removed.select_dtypes('number') numeric_names = numeric_feature.columns numeric_name 根据特征的说明文档，MSSubClass（房屋类型）、OverallQual（房屋质量评分）、OverallCond（房屋环境评分）应划分为类别型特征： X_data_removed[['MSSubClass','OverallQual','OverallCond']] = X_data_removed[['MSSubClass', 'OverallQual','OverallCond']].astype(str) 特征构建 我们可以利用原有的特征构建新特征： #构建房屋总面积 X_data_removed['TotalHouseArea'] = X_data_removed['TotalBsmtSF'] + X_data_removed['1stFlrSF'] + X_data_removed['2ndFlrSF'] #构建房屋翻修年限 X_data_removed['YearsSinceRemodel'] = X_data_removed['YrSold'] - X_data_removed['YearRemodAdd'] #构建房屋建造年限 X_data_removed['YrBlt'] = X_data_removed['YrSold'] - X_data_removed['YearBuilt'] #构建总浴室数量 X_data_removed['Total_Bathrooms'] = (X_data_removed['FullBath'] + (0.5 * X_data_removed['HalfBath']) + X_data_removed['BsmtFullBath'] + (0.5 * X_data_removed['BsmtHalfBath'])) #构建门廊总面积 X_data_removed['Total_porch_sf'] = (X_data_removed['OpenPorchSF'] + X_data_removed['3SsnPorch'] + X_data_removed['EnclosedPorch'] + X_data_removed['ScreenPorch'] + X_data_removed['WoodDeckSF']) 特征规整 1、标准化 标准化使不同维度上的特征在数值上更具可比性，能够提升模型的精度。 这里只对数值型数据进行标准化处理： #筛选数值型数据 num = X_data_removed.select_dtypes('number') num_names = num.columns #引入标准化工具并进行标准化 from sklearn.preprocessing import MinMaxScaler #创建对象，限定范围（1，2） scaler = MinMaxScaler(feature_range=(1, 2)) X_data_removed[num_names] = scaler.fit_transform(X_data_removed[num_names]) X_data_removed[numeric_names].head() 标准化后，数值的大小被控制在1到2之间。 2、非正态分布转换 正态分布的数据能更好地提高模型的准确度，因此我们要对非正态分布的数据进行转换。 from scipy.stats import norm, skew from scipy import stats,special #查看特征值的偏度分布 def sKewNess(data): skewed = data.apply(lambda x:skew(x)).sort_values(ascending=False) skewness = pd.DataFrame({'skew':skewed}) return skewness[skewness['skew'].abs()&gt;0.75] sKewNess(X_data_removed[num_names]) 可见，很多特征都不是正态分布的，我们可以尝试用box-cox对特征进行转换： #引用sklearn的PowerTransformer工具 from sklearn.preprocessing import PowerTransformer #box-cox方法只对正数生效，因而要配合上面的MinMaxScaler使用 power = PowerTransformer(method='box-cox') X_data_removed[numeric_names] = power.fit_transform(X_data_removed[numeric_names]) sKewNess((X_data_removed[num_names])) 得到了一定的改善，但部分数据因为自身的种种局限性仍保持较大的偏度。 #查看标签偏度 sns.distplot(y_train) 显然，房价不是正态分布。 #对房价进行正态分布转换 y_train = np.log1p(y_train) 3、编码与哑变量 许多算法模型只支持数值型的数据，因此我们需要把类别型数据转换成数值形式。 类别型数据又可以细分为两种类型：无序型（例如性别：男、女两个变量是独立的，没有顺序关系）和有序型（例如质量评分：1分和2分有顺序关系） #筛选类别型数据 X_data_removed[['OverallQual','OverallCond']] = X_data_removed[['OverallQual','OverallCond']].astype(int) cat_feature = X_data_removed.select_dtypes('object') cat_names = cat_feature.columns cat_names #区分有序、无序数据 ordinal_feature= ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC', 'FireplaceQu','GarageQual','GarageCond','PoolQC'] nominal_feature = [i for i in cat_names if i not in ordinal_feature] 诸如ExterQual（对外部环境的评分：poor、good、excellent）等特征都可归为有序特征，剩下的即为无序特征。 #排序 rank_dict = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5} for i in ordinal_feature: X_data_removed[i] = X_data_removed[i].map(rank_dict) X_data_removed[ordinal_feature].head() #用get_dummies函数将剩下的非数值型特征转化为哑变量 X_data_removed = pd.get_dummies(X_data_removed) X_data_removed.head() 上图只截取了SaleCondition生成的部分哑变量样本。 X_data_removed.shape (2832, 283) 可见，使用get_dummies生成哑变量后，维度由79增长到了283，维度越高模型越容易过拟合。 特征选择 特征数量过多可能导致模型过拟合，因此我们可以筛选出最重要的特征以降低特征维度，从而提升模型精度。 特征选择的方法有：相关系数法、互信息法、PCA主成分分析和递归特征消除法等，这里我们仅选择前两种进行分析。 定义评分函数 定义评分函数方便我们比较不同选择方法的效果： #环境设置 from sklearn.model_selection import RepeatedKFold from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor from numpy import mean #分离训练集、测试集 X_train = X_data_removed.iloc[:len(y_train),:] X_test = X_data_removed.iloc[len(y_train):,:] #定义评分函数 model = RandomForestRegressor(random_state = 42) def evaluate(X_train,y_train): cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42) scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise') return mean(scores) #进行特征选择前的基本模型评分 evaluate(X_train, y_train) -0.017256 定义特征选择函数 相关系数法和互信息法都可以通过SelectKBest实现，因此我们可以定义函数方便转换： from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_regression from sklearn.feature_selection import mutual_info_regression def select_features(X_train, y_train, X_test,func,k): # k:283个特征中选取k个,score_func:调用的方法 fs = SelectKBest(score_func=func, k=k) fs.fit(X_train, y_train) # 转换训练集 X_train_fs = fs.transform(X_train) # 转换测试集 X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs （1）相关系数法 #填入f_regression来调用相关系数法 X_train_fsc, X_test_fsc, fsc = select_features(X_train, y_train, X_test,f_regression,280) evaluate(X_train_fsc, y_train) -0.017265 相较于选择前的 -0.017256，评分略有下降。 （2）互信息法 X_train_fsm, X_test_fsm, fsm = select_features(X_train, y_train, X_test,mutual_info_regression,280) evaluate(X_train_fsm, y_train) -0.017217 评分上升了，我们可以用GridSearchCV继续寻找最优的特征数量： from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42) model = RandomForestRegressor() fs = SelectKBest(score_func=mutual_info_regression) pipeline = Pipeline(steps=[('sel',fs), ('rf', model)]) grid = dict() #sel表示SelectKBest，__k表示SelectKBest的参数k；特征选取范围（253-283） grid['sel__k'] = [i for i in range(X_train.shape[1]-30, X_train.shape[1]+1)] # 定义GridSearchCV search = GridSearchCV(pipeline, grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv) # 运行GridSearchCV results = search.fit(X_train, y_train) print('Best MSE: %.3f' % results.best_score_) print('Best Config: %s' % results.best_params_) Best MSE: -0.017 Best Config: {'sel__k': 282} 可见，选取282个特征时评分最高，但维度仍然较高，接下来尝试一下PCA法。 （3）PCA（主成分分析） from sklearn.decomposition import PCA #指定降维后的主成分方差和比例为99% pca_model = PCA(n_components = 0.99) X_train_de = pca_model.fit_transform(X_train) X_test_de = pca_model.transform(X_test) X_train_de.shape (1383, 124) evaluate(X_train_de,y_train) -0.017633 PCA法保留了124个特征，但评分并不好。 最终，我们选择评分较好的互信息法的282个特征用于预测： X_train_mb,X_test_mb,fs_mb =select_features(X_train, y_train, X_test,mutual_info_regression,282) 模型构建 from numpy import mean, std from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.svm import SVR from sklearn.kernel_ridge import KernelRidge from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor from sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold from sklearn.model_selection import RepeatedKFold from sklearn.ensemble import StackingRegressor 使用堆叠法： 堆叠法是一种整合多个模型为一个集成模型从而提高预测精度的方法。集成模型一般分两层，第一层会选取几个强力模型作为基本模型，如随机森林、XGboost等，第二层则会选择比较简单的模型，如linearRegressor,详情见： 详解stacking过程 def get_stacking(): # 构建基本模型 level0 = list() level0.append(('lgbm', LGBMRegressor())) level0.append(('Forest', RandomForestRegressor())) level0.append(('svm', SVR())) level0.append(('GDB', GradientBoostingRegressor())) level0.append(('XGB', XGBRegressor(objective ='reg:squarederror'))) # 构建次级模型 level1 = KernelRidge() # define the stacking ensemble model = StackingRegressor(estimators=level0, final_estimator=level1, cv=10) return model 引进其它模型： def get_models(): models = dict() models['knn'] = KNeighborsRegressor() models['cart'] = DecisionTreeRegressor() models['svm'] = SVR() models['k'] = KernelRidge() models['Forest'] = RandomForestRegressor() models['GDB'] = GradientBoostingRegressor() models['Extra'] = ExtraTreesRegressor() models['XGB'] = XGBRegressor(objective ='reg:squarederror') models['LGBM'] = LGBMRegressor() models['stacking'] = get_stacking() return models 定义评分函数： def evaluate_model(model): cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X_train_mb, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise') return scores 模型比较： models = get_models() #遍历，评估各模型性能 results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model) results.append(scores) names.append(name) print('&gt;%s %.3f (%.3f)' % (name, mean(scores), std(scores))) knn -0.024 (0.005) cart -0.037 (0.007) svm -0.013 (0.003) k -0.017 (0.006) Forest -0.017 (0.004) GDB -0.014 (0.004) Extra -0.015 (0.004) XGB -0.014 (0.003) LGBM -0.015 (0.004) stacking -0.012 (0.003) 集成模型的表现最优异，我们将用它作为预测模型： model = get_stacking() model.fit(X_train_mb,y_train) result = model.predict(X_test_mb) #因为之前将房价进行了正态分布转换，现在要进行还原 result = np.expm1(result) #导出结果 r=pd.DataFrame() r['Id']=test_data['Id'] r['SalePrice']=result #将预测结果导出为csv文件 r.to_csv(&quot;C:\\\\Users\\\\Steven\\\\Desktop\\\\Kaggle\\\\housing2.csv&quot;, index = False) ","link":"https://stdasein.life/post/fang-jie-yu-ce/"},{"title":"爬虫基本流程——爬取豆瓣电影top250","content":"设置环境 import bs4 import re import urllib.request,urllib.error import xlwt import sqlite3 from bs4 import BeautifulSoup import pandas as pd url = 'https://movie.douban.com/top250' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'} data = bytes(urllib.parse.urlencode({'a':'b'}),encoding = 'utf-8') req = urllib.request.Request(url = url,headers = headers) response = urllib.request.urlopen(req) print(response.read().decode('utf-8')) def asKurl(url): headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'} req = urllib.request.Request(url=url,headers=headers) response = urllib.request.urlopen(req) html ='' html = response.read().decode('utf-8') return html baseurl = 'https://movie.douban.com/top250?start=' relink = re.compile(r'&lt;a href=&quot;(.*?)&quot;&gt;') reimage = re.compile(r'&lt;img.*src=&quot;(.*?)&quot;',re.S) #re.S让换行符包含在字符中 retitle = re.compile(r'&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;') rerating = re.compile(r'&lt;span class=&quot;rating_num&quot; property=&quot;v:average&quot;&gt;(.*)&lt;/span&gt;') renum_comment = re.compile(r'&lt;span&gt;(\\d*)人评价&lt;/span&gt;') reqinq = re.compile(r'&lt;span class=&quot;inq&quot;&gt;(.*)&lt;/span&gt;') reinfo = re.compile('&lt;p class=&quot;&quot;&gt;(.*?)&lt;/p&gt;',re.S) def get_all(baseurl): datalist=[] for i in range(0,10): url = baseurl + str(i*25) html = asKurl(url) #逐一解析数据 soup = BeautifulSoup(html,'html.parser') for item in soup.find_all('div',class_='item'): data = [] item = str(item) link = re.findall(relink,item)[0] data.append(link) image = re.findall(reimage,item)[0] data.append(image) titles = re.findall(retitle,item) if (len(titles)==2): ctitle = titles[0] data.append(ctitle) ftitle = titles[1].replace('/','') data.append(ftitle) else: data.append(ctitle) data.append(' ') rating = re.findall(rerating,item)[0] data.append(rating) num_comment = re.findall(renum_comment,item)[0] data.append(num_comment) inq = re.findall(reqinq,item) if (len(inq) != 0): inq = inq[0].replace('。','') data.append(inq) else: data.append(' ') info = re.findall(reinfo,item)[0] info = re.sub('&lt;br(\\s+)?/&gt;(\\s+)?',' ',info) info = re.sub('/',' ',info) data.append(info.strip()) datalist.append(data) return datalist datalist = get_all(baseurl) import xlwt def saveData(datalist, savepath): workbook = xlwt.Workbook(encoding = 'utf-8') worksheet = workbook.add_sheet('sheet1') cols = ['电影链接','图片链接','中文名称','英文名称','评分','评价人数','评语','其他信息'] for i in range(0,8): worksheet.write(0,i,cols[i]) for i in range(0,250): data = datalist[i] for j in range(0,8): worksheet.write(i+1,j,data[j]) workbook.save(savepath) def main(): baseurl = 'https://movie.douban.com/top250?start=' #1.爬取网页并解析数据 datalist = get_all(baseurl) #2.保存数据 savepath = r'C:\\Users\\Steven\\Desktop\\ipython\\豆瓣电影.xls' saveData(datalist, savepath) import pymysql config = { 'host':'127.0.0.1', 'user':'root', 'password':'cai19930727205', 'database':'python_db', 'port':3306 } def init_db(): con = pymysql.connect(**config) cur = con.cursor() sql = &quot;&quot;&quot; create table moviestop250( id INT auto_increment, info_link VARCHAR(100), im_link VARCHAR(100), cname VARCHAR(100), fname VARCHAR(100), score FLOAT, rate_num INT, introduction TEXT, info TEXT, primary key(id) ) &quot;&quot;&quot; cur.execute(sql) cur.close() con.close() def saveData2DB(datalist): init_db() con = pymysql.connect(**config) cursor = con.cursor() for data in datalist: for i in range(len(data)): data[i] = '&quot;'+data[i]+'&quot;' sql =&quot;&quot;&quot; insert into moviestop250(info_link,im_link,cname,fname,score,rate_num,introduction,info) values(%s)&quot;&quot;&quot;%&quot;,&quot;.join(data) cursor.execute(sql) con.commit() cursor.close() con.close() saveData2DB(datalist) ","link":"https://stdasein.life/post/pa-chong-ji-ben-liu-cheng-yi-dou-ban-dian-ying-top250-wei-li/"},{"title":"电商指标分析","content":"目录： 一、数据概况 二、提出问题 三、分析思路 四、数据清理 五、数据分析 六、增长建议 一、数据概况 数据来源： 阿里天池 数据内容： 数据集提供了“双十一”期间部分卖家及其相应新买家的数据，包括这些新买家在前半年的购物行为日志，以及基本用户信息，所以数据集提供的用户均在”双十一“期间有购买行为。 二、提出问题 用户方面： 网站流量状况如何，在获客方面是否存在问题？ 新用户留存状况，用户忠诚度如何？ 获取营收状况？放弃购买的用户在哪个环节流失？ 怎样通过用户分层进行精细化运营？ 商品方面： 哪些商品销量更高？ 商品之间是否存在购买的关联性？ 卖家方面： 成功的卖家有什么特点？是否有值得借鉴之处？ 三、分析思路 使用人货场的分析框架并结合AARRR指标体系。 采用多维度拆解分析方法对问题进行拆解，用假设检验分析法、对比分析法和kmeans算法具体分析用户使用流程及具体业务指标中的问题。 四、数据清理 数据清理的步骤： 1.数据加载、合并 使用pandas载入数据集： import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns path_user_info = 'user_info_format1.csv' path_user_log = 'user_log_format1.csv' user_info = pd.read_csv(path_user_info) user_log = pd.read_csv(path_user_log) 1.1 查看数据 分别查看user_info和user_log的内容： （1）user_info #查看user_info前五条数据 user_info.head() user_info包含了用户的基本信息： user_id； age_range: 1 for &lt;18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for &gt;= 50; 0 and NULL for unknown； gender: 0 for female, 1 for male, 2 and NULL for unknown; （2）user_log #查看user_log前五条内容 user_log.head() user_log是用户的行为记录： item_id：商品的编号 cat_id：商品的种类 seller_id：商家的id brand_id：商品的品牌 time_tamp：行为发生的时间（format:mmdd) action_type：0表示点击，1表示加入购物车，2表示购买，3表示收藏 1.2 合并数据 为了方便分析，将user_info和user_log进行合并： user_full = pd.merge(user_log,user_info) #查看数据量 user_full.shape (54925330, 9) 2.缺失值处理 2.1 查看缺失值分布 #计算缺失值的数量 count_null = user_full.isnull().sum().sort_values(ascending=False) #缺失值占比 ratio = count_null/len(user_full) count_ratio = pd.concat([count_null,ratio],axis=1,keys = ['count','ratio']) count_ratio[count_ratio.ratio&gt;0] 可见，缺失值的占比较小，且只分布在性别、年龄和品牌三列。 2.2 填充缺失值 因为数据集里性别未知表示为NaN或2，所以我们将缺失值统一填充为2；同理，将年龄缺失值填充为0。 #填充年龄、性别的缺失值 user_full['age_range'].fillna(0,inplace=True) user_full['gender'].fillna(2,inplace=True) #对品牌的缺失值填充‘Missing' user_full['brand_id'].fillna('Missing',inplace=True) user_full.isnull().sum() user_id 0 item_id 0 cat_id 0 seller_id 0 brand_id 0 time_stamp 0 action_type 0 age_range 0 ender 0 dtype: int64 3.重复值处理 #选择重复的行 count_dup = user_full[user_full.duplicated()] count_dup.count() user_id 2498641 item_id 2498641 cat_id 2498641 seller_id 2498641 brand_id 2498641 time_stamp 2498641 action_type 2498641 age_range 2498641 gender 2498641 dtype: int64 查询发现，数据集有2,498,641行重复数据，这个数字非常大，不能简单的认为是数据的冗余，可能确实是用户重复操作所导致的，对此我们决定予以保留。 4.时间格式转换 将原本的时间戳转换成更直观的形式。 拆分出月和日，方便后续的分析。 #引入datetime工具库转换时间格式 import datetime as dtimport datetime user_full['time_stamp'] = pd.to_datetime(user_full['time_stamp'],format='%Y%m%d') #拆分月、日 user_full['month'] = user_full['time_stamp'].dt.month.astype(str) user_full['day'] = user_full['time_stamp'].dt.day.astype(str) user_full['year'] = '2015' user_full['date'] = user_full['year'] + user_full['month'] + user_full['day'] user_full['date'] = pd.to_datetime(user_full['date'],format='%Y%m%d') user_full.drop(['time_stamp'],axis=1,inplace=True) user_full.head() 五、数据分析 这部分将会对数据集进行具体分析。 1.人 本环节关注点是用户的行为习惯，利用AARRR的部分因素、转化率概念和kmeans模型对用户行为进行分析。 Acquisition：客户获取 指标选取 pv：即Page View，网站浏览量，指页面浏览的次数，用以衡量网站用户访问的网页数量。用户每次打开一个页面便记录1次PV，多次打开同一页面则浏览量累计 uv：即Unique Visitor，独立访客数，指一天内访问某站点的人数，以cookie为依据。1天内同一访客的多次访问只记录为一个访客。 指标分析 按月分组： group_m = user_full.groupby('month') group_m.get_group(5).head() 月流量对比分析： #创建函数计算pv,uv和pv/uv def volumeFlow(x): month = group_m.get_group(x) uv = month['user_id'].nunique() pv = month[month['action_type']== 0]['action_type'].count() return pv,uv,pv/uv keys = ['May','Jun','Jul','Aug','Sep','Oct','Nov'] values = [] #遍历5—11月并将数据放入列表 for i in range(5,12): a = volumeFlow(i) values.append(a) #转化为DataFrame dictionary = dict(zip(keys, values)) df = pd.DataFrame(dictionary,index=['pv','uv','pv/uv']).astype(int) df.columns.name = 'Month' df.index.name = 'indicator' df 可视化： #可视化 pv = df.iloc[0,:].values uv = df.iloc[1,:].values pv_uv = df.iloc[2,:].values x = ['May','Jun','Jul','Aug','Sep','Oct','Nov'] fig,(ax1,ax2,ax3) = plt.subplots(3,1) ax1.plot(x,pv,'o-') ax1.set_ylabel('pv') ax2.plot(x,uv,'.-') ax2.set_ylabel('uv') ax3.plot(x,pv_uv) ax3.set_ylabel('pv/uv') plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35) 分析总结 整体上看，五月到十一月的浏览量呈现出增长的趋势，增长可归因于两方面： 独立访客(uv)的增加： 原因假设：1）新客增加；2）旧客唤醒； 验证思路：对用户按首次使用日期进行群组分析，区分新旧用户，计算新客的增长情况；按用户的来源渠道进行分组，筛选有效的获新/唤醒渠道。（新旧用户的分析将会在留存率环节展开所以在此不过多表述，而来源渠道分析因为资料不足无法进行，在此只提供思路。） 人均浏览量的增加： 原因假设：网页内容优化、推荐算法提升和优惠活动等 十一月份出现流量峰值： 原因假设：双十一促销的影响 假设检验：分析对比双十一前后流量的变化，一般而言，促销过后流量会有一定程度的回落。 收集证据： #抓取11月数据 group_nov = group_m.get_group(11) group_nov = pd.DataFrame(group_nov) #查看11月天数分布 group_nov.day.value_counts() 11 1921587 10 531651 9 213696 8 173739 7 156503 6 146463 5 134096 4 130784 3 108929 1 101456 2 99219 12 11 Name: day, dtype: int64 11月12日的数据量太小，我们将其作为异常值剔除。 group_d = group_nov.groupby('day') #定义函数计算每天的流量 def getDailyVolumeFlow(x): day = group_d.get_group(x) uv = day['user_id'].nunique() pv = day[day['action_type']== 0]['action_type'].count() return pv,uv,pv/uv keys_d = list(range(1,12)) values_d = [] #将每天的流量放进列表中 for i in range(1,12): #不包含第十二天的异常值 a = getDailyVolumeFlow(i) values_d.append(a) #转换为便于观察的DataFrame形式 dictionary_d = dict(zip(keys_d, values_d)) df_day = pd.DataFrame(dictionary_d,index=['pv','uv','pv/uv']).astype(int) df_day 结论： 从统计数字可以看出，越接近双十一，网站的流量越大，而双十一当天的流量甚至比以往一个月的流量都还要大。可见，打造消费文化对消费的强大促进作用。 继续提问：双十一过后的流量还能保持在这个水平吗？会不会透支了用户未来的消费潜力？ 验证思路：可通过零售中促销爆发度和促销衰减度的指标进行分析对比，如果衰减度大于爆发度则有销售透支的现象发生，如果衰减度大于两倍的爆发度，那这个促销活动就是彻底失败了。 受限于手头上的数据，我们无法对双十一之后的流量进行分析。 Retention：客户留存 指标选取 留存率：用户在某段时间内开始使用应用，经过一段时间后，仍然继续使用该应用的用户，被认作是留存用户。这部分用户占当时新增用户的比例即是留存率，会按照每隔1单位时间（例日、周、月）来进行统计。 复购率：复购率就是重复购买的用户占所有存在购买行为用户的比率。 回购率：回购率就是在上个时间窗口有购买行为的同时在下个时间窗口仍然有购买行为的用户数量占上个窗口有购买行为用户的比例。 指标分析 考虑到电商的特殊性，我们以月为单位进行分析。 （1）留存率 留存率的分析思路：首先计算每个月的新增用户数量，再分析后续留存； 留存率=新增用户中登录用户数/新增用户数*100%； 新增用户数：在某个时间段新登录应用的用户数； 登录用户数：登录应用后至当前时间，至少登录过一次的用户数； 以六月份为例，演示留存率计算过程。 第一步：筛选出六月前就已经存在的用户，因为数据集的局限性，我们只能将五月份的数据视为全部的历史数据，所以五月份的用户就是计算新增用户的基准。 #计算基准用户数 may = user_full.loc[user_full['month']==5,:] print('五月份用户数量：%d'%may['user_id'].nunique()) 五月份用户数量：40235 五月份的用户数量为40,235，而在六月份用户中在五月份没出现过的部分，就是新增用户。要注意的是，因为五月份没有前置数据，所以只能将五月份的所有用户都视为新增用户。 有了基准，我们就可以计算出六月的新增用户数了。首先用loc截取六月份的全部数据，然后通过isin筛选新增用户。 #计算六月新增用户数 june = user_full.loc[user_full['month']==6,:] jun_new = june.loc[june['user_id'].isin(may['user_id'])==False,:] print('六月份新增用户数量：%d'%jun_new['user_id'].nunique()) 六月份新增用户数量：17402 六月份新增用户数量为17,402，接下来要计算的留存数量本质上就是在这个数字的基础上查看用户的留存情况，留存数量少于17,402，即出现了用户流失。 为了计算留存数量，我们仍然会使用isin进行筛选，不过方向相反。 retention = [] m_list = [7,8,9,10,11] #遍历计算六月新用户的后续留存状况 for i in m_list: m_next = user_full.loc[user_full['month']==i,:] m_re = m_next.loc[m_next['user_id'].isin(jun_new['user_id'])==True,:] retention.append('%d月份留存: %d'%(i,m_re['user_id'].nunique())) #插入六月份新增用户数 retention.insert(0,'6月份新增用户：%d'%jun_new['user_id'].nunique()) retention ['6月份新增用户：17402', '7月份留存: 11418', '8月份留存: 10977', '9月份留存: 12199', '10月份留存: 13575', '11月份留存: 17402'] 结果显示，六月份新增用户为17,402，八月留存10,977，流失率达到最大，往后几个月逐渐回升。 现在，我们会通过遍历来计算所有月份的留存状况。外层的遍历计算每个月的新增用户数，内层的遍历计算留存数。 month = list(range(5,12)) df_re = pd.DataFrame() #计算每月新增用户 for i in range(len(month)-1): #每次形成一列，放置新增用户数及以后的留存数 count = [0]*len(month) target_month = user_full.loc[user_full['month']==month[i],:] #计算五月份新用户数量，因为五月份无前置月份，所以跳过筛选 if i == 0: new_users = target_month new_users_num = new_users['user_id'].nunique() else: history_month = user_full.loc[user_full['month'].isin(month[:i]),:] new_users = target_month.loc[target_month['user_id'].isin (history_month['user_id'])==False,:] new_users_num = new_users['user_id'].nunique() #储存新用户数量，放在每行的第一个位置 count[0] = new_users_num #同时遍历，j遍历月份，c遍历列表位置 for j,c in zip(range(i + 1,len(month)),range(1,len(month))): next_month = user_full.loc[user_full['month']==month[j],:] next_users = next_month.groupby('user_id')['action_type'].sum().reset_index() #计算留存数 re_next_users = next_users.loc[next_users['user_id'].isin (new_users['user_id'])==True,:] re_next_users_num = re_next_users['user_id'].nunique() #储存留存数 count[c] = re_next_users_num #合并 result = pd.DataFrame({month[i]:count}).T df_re = pd.concat([df_re,result]) df_re.columns = ['新增用户','+1月','+2月','+3月','+4月','+5月','+6月'] df_re.index.name ='月份' df_re 注意：所有月份的留存率在11月都达到100%，这是因为数据集提供的用户均在”双十一“期间有购买行为。 #单独计算11月新增用户数 month_2 = list(range(5,12)) target_month = user_full.loc[user_full['month']==month_2[6],:] history_m_to_d = user_full.loc[user_full['month'].isin(month_2[:6]),:] new_users = target_month.loc[target_month['user_id'].isin (history_m_to_d['user_id'])==False,:] new_users_num_nov = new_users['user_id'].nunique() print('十一月份新增用户数量：%d'%new_users_num_nov) 十一月份新增用户数量：1582 6至11月份，新增用户呈现逐月递减的趋势。 我们把留存数量转化为比率并进行可视化，方便对比分析。 re_table = df_re.divide(df_re['新增用户'],axis=0) re_ratio = re_table.drop(['新增用户'],axis=1) re_ratio.index.name = 'month' re_ratio.columns = ['+1m','+2m','+3m','+4m','+5m','+6m'] #生成热力图 sns.heatmap(re_ratio,annot=True, fmt=&quot;.0%&quot;) 分析小结： 新增用户数量逐月递减： 原因假设：单一获客渠道饱和； 验证思路：根据获客渠道，年龄等不同属性对用户进行群组分析，分析用户成分，如果成分单一，就可能意味着饱和，应尝试开拓新的渠道。 7月的留存率相较于其他月份偏低，仅有59%： 原因假设：网页、app改版，影响用户体验。 验证思路：对流失用户进行问卷调查、AB测试 留存率基本保持在60%以上，高于电商行业标准的5—8%； （2）复购率 复购率分析思路：筛选出具有购买行为的用户，然后通过透视图统计各用户购买次数。 #筛选所有购买行为 user_buy = user_full[user_full['action_type']==2] user_buy.shape (608290, 12) 5至11月间共发生了608,290次购买行为。 用透视表按月份统计每个用户的购买次数: #用透视表按月份统计每个有购买行为的用户的购买次数 pivot = user_buy.pivot_table(index = 'user_id', columns = 'month',values ='action_type', aggfunc='count') pivot.head() 如上图所示，在当月没有购买行为的用户填入NaN。 #为了方便求出复购率，对透视表进行转化 pivot = pivot.fillna(0) pivot_tr = pivot.applymap(lambda x:1 if x&gt;1 else 0 if x == 1 else np.nan) pivot_tr.head() #求复购率，用sum统计购买次数大于1的用户数量，用count统计所有购买用户数 ratio = pivot_tr.sum()/pivot_tr.count() ratio #复购率： month 5 0.518338 6 0.540954 7 0.518946 8 0.516869 9 0.549189 10 0.565497 11 0.692119 dtype: float64 #可视化 plt.plot(ratio) plt.xlabel('month') plt.ylabel('repurchase rate') plt.suptitle('monthly repurchase rate') 分析小结： 各月份复购率均保持在50%以上，用户忠诚度高。 8—11月复购率持续增长： 原因假设：具有购买行为的用户数量下降，分母变小，导致复购率增加。 假设检验：分析购买用户数量。 收集证据： #查看各统计指标 pivot.describe() 整体上，每月具有购买行为的用户数量呈现上升趋势，其中5-10月增长比较平缓。 结论：假设不成立，复购率提升不是因为用户的流失。 （3）回购率 回购率分析思路：与复购率类似但更复杂一些，需要考虑下一个窗口期间的购买情况。 #生成透视表，统计每月购买情况 pivot_rep = user_buy.pivot_table(index = 'user_id',columns = 'month',values = 'action_type',aggfunc = 'count',fill_value = 0) #数据转换，将存在购买行为的情况用1表示，不存在的用0表示 pivot_rep_tr = pivot_rep.applymap(lambda x: 1 if x&gt;0 else 0) pivot_rep_tr.head() #定义函数转化透视表每行数值：本月与下月同时为1（购买）的，在本月生成1;本月与下月分布为1,0（本月有下月没有购买），在本月生成0；本月为0，则在本月生成nan. def transform(data): trans_list = [] for i in range(5,11): if data[i] == 1: if data[i + 1] == 1: trans_list.append(1) if data[i + 1] == 0: trans_list.append(0) else: trans_list.append(np.NaN) #因为11月没有后续月份，所以填入nan trans_list.append(np.NaN) return pd.Series(trans_list) #应用函数，apply表示使用函数转化前面的DataFrame,axis=1表示对象是整行 final_pivot = pivot_rep_tr.apply(transform,axis=1) #对列重命名 final_pivot.columns = range(5,12) final_pivot.head() #计算回购率 re_ratio = final_pivot.sum()/final_pivot.count() re_ratio #回购率： 5 0.513516 6 0.454410 7 0.463757 8 0.522471 9 0.565885 10 0.999968 11 NaN dtype: float64 #可视化 # 指定默认字体 plt.rcParams['font.sans-serif'] = ['KaiTi'] # 解决保存图像是负号'-'显示为方块的问 plt.rcParams['axes.unicode_minus'] = False 题 plt.plot(re_ratio) plt.xlabel('月份') plt.ylabel('回购率') plt.suptitle('7个月内回购率图') 分析小结： 回购率整体上分布在50%上下五个点的范围内，客户忠诚度高。 十月份的回购率达到了100%，说明了双十一的强大影响。 总结 从指标上看，留存率、复购率和回购率基本保持在50%以上，平台用户的忠诚度颇高，但同时也存在以下问题： 新用户增长缓慢、乏力。 购物狂欢节（618、双十一）可以有效拉动指标增长，但可能不具有持续性。 建议： 开拓新市场，尝试新的获客渠道；增强平台的社交属性，打造购买—分享—交友的生态圈。 可以用机器学习辨识预测哪些用户在节日过后更可能留存，有针对性地进行营销以降低成本。 Revenue：营收获取 假设1：路径越短，越有利于提高购买转化率。 假设2：特定的中间环节（加购、收藏）可以提高购买转化率。 验证思路：分析用户各路径的购买转化率，共有8条路径。 浏览—&gt;流失 浏览—&gt;购买 浏览—&gt;收藏—&gt;购买 浏览—&gt;收藏—&gt;流失 浏览—&gt;加入购物车—&gt;购买 浏览—&gt;加入购物车—&gt;流失 浏览—&gt;加入购物车、收藏—&gt;购买 浏览—&gt;加入购物车、收藏—&gt;流失 收集证据： 区分用户的不同行为路径 #去重 user_full_con = user_full.drop_duplicates(['user_id','item_id','action_type']) #将购买行为标记为10，以区分其他行为 user_full_con['action_type'] = user_full_con['action_type'].apply(lambda x: 10 if x == 2 else x) #按用户、商品分组对用户行为进行求和 group_con = user_full_con[['user_id','item_id', 'action_type']].groupby(['user_id','item_id']).sum() group_con.head() #浏览—&gt;流失标记为10 pv_buy = group_con[group_con['action_type'] == 10] #浏览—&gt;购买标记为0 pv_unbuy = group_con[group_con['action_type'] == 0] #浏览—&gt;收藏—&gt;购买标记为13 pv_fav_buy = group_con[group_con['action_type'] == 13] #浏览—&gt;收藏—&gt;流失标记为3 pv_fav_unbuy = group_con[group_con['action_type'] == 3] #浏览—&gt;加入购物车—&gt;购买标记为11 pv_cart_buy = group_con[group_con['action_type'] == 11] #浏览—&gt;加入购物车—&gt;流失标记为1 pv_cart_unbuy = group_con[group_con['action_type'] == 1] #浏览—&gt;加入购物车、收藏—&gt;购买标记为14 pv_fav_cart_buy = group_con[group_con['action_type'] == 14] #浏览—&gt;加入购物车、收藏—&gt;流失标记为4 pv_fav_cart_unbuy = group_con[group_con['action_type'] == 4] 计算不同路径的数量： list_action = [pv_buy,pv_unbuy,pv_fav_buy,pv_fav_unbuy, pv_cart_buy,pv_cart_unbuy,pv_fav_cart_buy,pv_fav_cart_unbuy] def act_num(data): act_list = [] for i in range(8): act = data[i].shape[0] act_list.append(act) return act_list pv_buy 484778 pv_unbuy 4789692 pv_fav_buy 55924 pv_fav_unbuy 477300 pv_cart_buy 2086 pv_cart_unbuy 9898 pv_fav_cart_buy 169 pv_fav_cart_unbuy 474 dtype: int64 结论：从总环节转化率来看，假设1成立；从单环节转化率来看，假设2成立 分析发现： 大部分用户（82%）在浏览后就会流失掉。 从总环节转化率来看，浏览—&gt;购买路径的转化率最高（8.329%）。 从单环节转化率来看，浏览—&gt;收藏、加购路径的转化率最高（26.3%），其次是浏览—&gt;加购路径（17.4%），同时，要注意的是，虽然浏览—&gt;收藏路径的转化率最低（10.5%），但其绝对人数（533224）远大于前两者。 建议： 针对用户浏览后迅速流失：调查流失原因。如果是推荐的商品不合适，可着手优化推荐系统；如果是网站引导有所欠缺，可进行相关的改善；如果是拉新渠道不对，获取的不是目标用户，可做相应调整。 浏览—&gt;购买路径的总环节转化率最高，说明路径越短，用户购买的可能性越高，因为更长的路径有可能冷却用户的购买热情。因此，可继续简化通向最终购买的环节。 可以使用触发物激活加购、收藏后不购买的用户，例如赠予优惠券等。 Kmeans模型：用户分层 分析思路与特征选取： 使用Kmeans算法，基于最近一次消费的时间间隔、购买频率、年龄和性别对用户进行分群。 最近一次消费的时间间隔：假设今天是2015年12月12日，最近一次消费的时间间隔就是今天与最近一次消费时间的差值。 购买频率：用户的购买次数。 年龄和性别 整理时间序列 #为方便求出最近一次消费的时间间隔，对数据的时间序列进行整理 #假设今天为2015年12月12日 now = dt.datetime(2015,12,12) #筛选出有购买行为的数据，同时剔除双十一的影响 user_buy = user_full[(user_full['action_type']==2) &amp; (user_full['month'] != 11)] user_buy.drop(['month','day','year'],axis=1,inplace=True) user_buy.head() 计算时间间隔： #求最近一次消费的时间间隔 user_buy['time_interval'] = now - user_buy['date'] #转换日期格式 user_buy['time_interval'] = user_buy['time_interval'] / np.timedelta64(1,'D') user_buy.head() 提取特征： #按用户分组，聚合取得最近购买距今天数、购买频率、年龄、性别 user_RFAG = user_buy.groupby('user_id').agg({'time_interval':np.min, 'user_id':np.size, 'age_range':np.max,'gender':np.average}) user_RFAG.rename(columns = {'time_interval':'recency', 'user_id':'frequency'},inplace = True) user_RFAG.head() 寻找最佳分组数量： from sklearn.cluster import KMeans #遍历k，找合适的k值，k表示将用户划分为k组 inertia = [] for i in range(1,15): model = KMeans(n_clusters=i,n_jobs=4) model.fit(user_RFAG) inertia.append(model.inertia_) plt.plot(range(1,15),inertia,marker='o') plt.xlabel('number of clusters') plt.ylabel('distortions') plt.show() 可见，k=3时，误差相对较低，应将客户划分为3组。 建立模型并分组： #建立模型并进行分群 k = 3 kmodel=KMeans(n_clusters=k,n_jobs=4) kmodel.fit(user_RFAG) #将结果合并 r1=pd.Series(kmodel.labels_).value_counts() r2=pd.DataFrame(kmodel.cluster_centers_) r3=pd.Series(['group1','group2','group3']) r=pd.concat([r3,r1,r2],axis=1) r.columns=['聚类类别','聚类个数']+list(user_RFAG.columns) r.sort_values(by='聚类个数',ascending = False).assign(ratio = r['聚类个数']/r['聚类个数'].sum()) 分析小结： group3购买间隔最短，频率最高，应划分为重要价值用户，可提供vip服务。 group1购买间隔、频率中等，应划分为一般价值用户，可通过触发物（例如推送告知特价商品、优惠活动等）提升其购买频率。 group2为普通用户，可与之保持联系，积极调查流失原因。 2.货 这部分将分析商品/商品种类的销量和销售关联性。 商品销量分析 （1）查看销售的总体情况 查看销量前十的商品： item_buy = user_buy[['item_id','action_type']].groupby('item_id').count().reset_index() item_buy = item_buy.rename(columns={'action_type':'buy_times'}) item_buy.sort_values(by = 'buy_times',ascending = False).head(10) 查看销量分布情况： item_buy_count = item_buy[['item_id','buy_times']].groupby('buy_times').count() item_buy_count = item_buy_count.rename(columns = {'item_id':'item_num'}) item_buy_count_head = item_buy_count.sort_values(by = 'item_num', ascending = False).head(10) item_buy_count_head.div(item_buy_count['item_num'].sum()) from matplotlib.pyplot import MultipleLocator plt.plot(item_buy_count_head) plt.xlabel('购买次数') plt.ylabel('商品数量') plt.suptitle('不同购买次数的商品数量分布') x_major_locator=MultipleLocator(1) ax=plt.gca() ax.xaxis.set_major_locator(x_major_locator) 分析小结： 销量最高的商品总共售出了846件，相比之下，接近80%的商品只出售过1到3次，呈现出长尾效应。 （2）分析商品销量的成因 假设：销量与流量、加购量、收藏量等因素相关 验证思路：查看每个商品对应的点击、购买、加购和收藏量，分析其中的联系。 收集证据： #筛选不同行为的数据 user_pv = user_full[user_full['action_type']==0] user_cart = user_full[user_full['action_type']==1] user_fav = user_full[user_full['action_type']==3] #按商品分组，查看各商品不同行为的数量 item_pv = user_pv[['item_id','action_type']].groupby('item_id').count() item_pv = item_pv.rename(columns = {'action_type':'pv_num'}) item_cart = user_cart[['item_id','action_type']].groupby('item_id').count() item_cart = item_cart.rename(columns = {'action_type':'cart_num'}) item_fav= user_fav[['item_id','action_type']].groupby('item_id').count() item_fav = item_fav.rename(columns = {'action_type':'fav_num'}) item_buy = item_buy.set_index('item_id') #将各行为数量合并 item = pd.merge(item_buy,item_pv,on = 'item_id',how = 'inner') item = pd.merge(item,item_cart,on = 'item_id',how = 'inner') item = pd.merge(item,item_fav,on = 'item_id',how = 'inner') #查看销量前十的商品 item.sort_values(by = 'buy_num', ascending = False).head(10) #查看浏览量前十的商品 item.sort_values(by = 'pv_num', ascending = False).head(10) #查看加购量前十的商品 item.sort_values(by = 'cart_num', ascending = False).head(10) #查看收藏量前十的商品 item.sort_values(by = 'fav_num', ascending = False).head(10) 结论： 流量前十的商品中只有一件位于销量前十，可能说明：1）用户本身需求并不多，但平台推荐多，会导致点击率高，但最终销量低 。2）用户本身有需求，而平台并没有推荐到位，用户通过自己搜索和寻找，促使该商品销量高。 加购和收藏次数前十的商品都只有两件位于销量前十，说明用户有购买的想法却因为种种原因没有购买； 建议： 改善推荐系统，推荐用户需要的商品，提高流量的转化率； 调查加购/收藏用户的流失原因，改善各环节的流畅度（例如支付环节是否太过繁琐）； 商品种类分析 分析商品各种类的销量： user_buy_cat = user_buy[['cat_id','action_type']].groupby('cat_id').count() user_buy_cat = user_buy_cat.rename(columns = {'action_type':'buy_count'}) user_buy_cat = user_buy_cat.sort_values(by = 'buy_count',ascending = False) user_buy_cat = user_buy_cat.reset_index() user_buy_cat.head() 可见，销量前五的商品种类差距并不大。 分析商品种类与用户行为的联系： #将具有购买行为的用户id和商品种类与全部数据合并，突显被购买商品种类对应的全部用户行为 user_behav_cat = pd.merge(user_buy[['user_id','cat_id']],user_full, on = ['user_id','cat_id'],how = 'left') #计算被购买商品种类对应的全部行为数 user_behav_cat = user_behav_cat[['cat_id','action_type']].groupby('cat_id').count() user_behav_cat = user_behav_cat.rename(columns ={'action_type':'behav_count'} ) user_behav_cat.sort_values(by = 'behav_count',ascending=False).head() #合并被购买商品种类的购买计数与行为计数 user_cat_buy_behav = pd.merge(user_buy_cat,user_behav_cat,on = 'cat_id',how = 'inner') user_cat_buy_behav = user_cat_buy_behav.reset_index() user_cat_buy_behav = user_cat_buy_behav.assign(ratio = user_cat_buy_behav['behav_count']/ user_cat_buy_behav['buy_count']) user_cat_buy_behav.head(10) #尾部商品种类 user_cat_buy_behav.tail(10) 分析小结： 头部商品种类销量大，但对应的行为—&gt;购买转化率较尾部种类低，平均20个行为转化为1个购买，而尾部商品种类的平均转化率为3：1 原因假设： 头部商品种类同质化较严重，竞争激烈，用户购买前需要更多的对比；而尾部商品种类则更具个性化，能更好地满足细分市场的需求。（因为数据不足无法继续论证） 建议： 尝试将热门商品种类与冷门商品种类捆绑销售，同时利用前者的高流量和后者的高转化。 注重长尾效应，积极开拓细分市场。 关联性分析 （1）分析各商品种类间的关联性 在购买行为中，按用户id和日期分组，生成订单 #计算总订单数 #假设一个用户一天内所有的购买都在一个订单 order_num = user_buy.groupby(['user_id','date']).count() order_num.shape[0] #总订单数 327130 #筛选出商品数量大于1的订单 order_items_num = order_num[order_num['item_id'] &gt; 1].reset_index() order_items_num = order_items_num[['user_id','date','item_id']].rename(columns={'item_id':'item_num'}) order_items_num.sort_values(by = 'item_num',ascending = False).head(10) 按销量排名前十的订单均发生在购物狂欢节。 在订单中添加商品id和种类： #合并，突显订单中的商品和商品种类 order_items_cat = pd.merge(order_items_num, user_buy, on = ['user_id','date'], how = 'inner') order_items_cat.head() 显示两两同时出现的商品种类： relation_cat_a = order_items_cat[['user_id','date','cat_id']] relation_cat_b = order_items_cat[['user_id','date','cat_id']] #自连接 relation = pd.merge(relation_cat_a,relation_cat_b, on = ['user_id','date'], how = 'left') #去重 relation = relation[relation['cat_id_x'] &lt; relation['cat_id_y']] relation.head() 通过计算种类两两出现的频率来表示种类之间的关联性： rel_count = relation[['cat_id_x','cat_id_y','user_id']].groupby(['cat_id_x','cat_id_y']).count() rel_count = rel_count.rename(columns = {'user_id':'relation'}).sort_values(by = 'relation',ascending = False) rel_count.reset_index().head(10) 分析小结： 某些商品种类之间存在关联性，用户购买了其中一种时可能也会购买另一种。 1213与420表现出与多个商品种类的强关联性 建议： 可根据商品种类间的关联性进行商品推荐 （2）分析各单独商品间的关联性 与种类关联性分析的方法基本一致 relation_item_a = order_items_cat[['user_id','date','item_id']] relation_item_b = order_items_cat[['user_id','date','item_id']] relation_item = pd.merge(relation_item_a,relation_item_b, on = ['user_id','date'], how = 'left') relation_item = relation_item[relation_item['item_id_x'] &lt; relation_item['item_id_y']] relation_item.head() rel_item_count = relation_item[['item_id_x','item_id_y','user_id']].groupby(['item_id_x','item_id_y']).count() rel_item_count = rel_item_count.rename(columns = {'user_id':'relation'}).sort_values(by = 'relation',ascending = False) rel_item_count.reset_index().head() 分析小结： 282032号和642180号商品具有最高的关联性。 1029992号商品和590204号商品表现出与多个商品的关联性。 3.场 卖家分析 分析各卖家流量与销量的联系 将用户对卖家商品的所有行为都视为卖家的流量。 #按卖家分组，统计商品数量 seller_group = user_full[['seller_id','item_id']].groupby('seller_id').count().reset_index() seller_group = seller_group.rename(columns = {'item_id':'item_count'}) seller_group.sort_values(by = 'item_count',ascending = False).head(10) #统计销售商品数量 seller_buy_group = user_buy[['seller_id','item_id']].groupby('seller_id').count().reset_index() seller_buy_group = seller_buy_group.rename(columns = {'item_id':'item_buy_count'}) #合并对比，算出转化率 seller_combined = pd.merge(seller_group,seller_buy_group,on = 'seller_id',how = 'left') seller_combined = seller_combined.assign(ratio = seller_combined['item_buy_count']/seller_combined['item_count']) seller_combined.sort_values(by = 'item_buy_count',ascending = False).head(10) #计算平均转化率 seller_combined['ratio'].mean() 0.0781 分析总结： 销量前十的卖家有六家同时也是流量前十的卖家，说明流量与销量正相关; 销量前十的高流量卖家转化率普遍较低，低于7.8%的平均水平，应该关注这些卖家的获客成本，分析其推广、营销费用是否过多，获客渠道是否合适等; 在销量前十的卖家中，1200号卖家以最低的流量获取了较高的销量，转化率一枝独秀，高达26.5%，应进一步研究: 原因假设：客户忠诚度高 验证思路：比较1200号卖家和销量第一的3828号卖家的复购率。 收集证据： #生成透视表，行索引为用户id,列索引为月份 seller_1200 = user_full[user_full['seller_id']==1200] seller_1200_buy = seller_1200[seller_1200['action_type']==2] pivot_1200_re = seller_1200_buy.pivot_table(columns='month',index='user_id', values='action_type',aggfunc='count') #转换数值以计算复购率 pivot_1200_re = pivot_1200_re.applymap(lambda x:1 if x&gt;1 else 0 if x == 1 else np.nan) #卖家1200复购率 ratio_1200 = pd.Series(pivot_1200_re.sum()/pivot_1200_re.count(),name = 're_ratio') #计算卖家3828复购率 seller_3828 = user_full[user_full['seller_id']==3828] seller_3828_buy = seller_3828[seller_3828['action_type']==2] pivot_3828_re = seller_3828_buy.pivot_table(columns='month',index='user_id',values='action_type',aggfunc='count') pivot_3828_re = pivot_3828_re.applymap(lambda x:1 if x&gt;1 else 0 if x == 1 else np.nan) ratio_3828 = pd.Series(pivot_3828_re.sum()/pivot_3828_re.count(),name = 're_ratio') #合并 ration_merge = pd.merge(ratio_1200,ratio_3828,on = 'month',how = 'inner') ration_merge = ration_merge.rename(columns = {'re_ratio_x':'re_ratio_1200','re_ratio_y':'re_ratio_3828'}) ration_merge.applymap(lambda x: format(x,'.2%')) 结论： 假设成立，1200号卖家的复购率整体上高于3828号卖家，如果其他条件不变，1200号卖家的顾客忠诚度更高。 六、增长建议 1.获取新用户方面： 积极探索开发主流人群（25-35岁女性）之外群体的商机，尝试比较不同的获客渠道，平衡成本与收益； 2.提高用户的体验与粘性： 探寻增强平台的社交属性，打造购买—分享—交友的生态圈； 可以通过机器学习、数据挖掘预测哪些用户在节日过后更可能留存，有针对性地进行营销； 优化推荐系统辅助用户决策； 3.促进用户交易： 简化通向购买行为的环节； 可以使用触发物激活加购、收藏后不购买的用户，例如赠予优惠券等； 积极向用户推荐关联性高的商品，促进连带销售； 尝试将热门商品与冷门商品捆绑销售，同时利用前者的高流量和后者的高转化； 4.用户画像/精准运营 针对用户的不同属性进行分层管理； ","link":"https://stdasein.life/post/dian-shang-zhi-biao-fen-xi/"}]}